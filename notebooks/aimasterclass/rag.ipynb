{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c31c3e8",
   "metadata": {},
   "source": [
    "# RAG from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cdbbf",
   "metadata": {},
   "source": [
    "- Adapted from https://github.com/opendatahub-io/llama-stack-demos/\n",
    "- Requires distributions/masterclass-agents/run.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8319431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:8321\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(identifier='llama3.2:3b-instruct-fp16', metadata={}, api_model_type='llm', provider_id='ollama', type='model', provider_resource_id='llama3.2:3b-instruct-fp16', model_type='llm'),\n",
       " Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='ollama', type='model', provider_resource_id='all-minilm:latest', model_type='embedding')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d21e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='my-vector-db', provider_id='faiss', type='vector_db', provider_resource_id='my-vector-db', owner={'principal': '', 'attributes': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_id = \"my-vector-db\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf72cc",
   "metadata": {},
   "source": [
    "Ingesting documents into a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaf4f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-0'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-1'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-2'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2066200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c68396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the top 5 topics that were explained? Only list succinct bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2328bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryResult</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span><span style=\"font-weight: bold\">]}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 1\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 2\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 3\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 4\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 5\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'END of knowledge_search tool results.\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 1\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 2\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 3\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 4\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 5\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtext\u001b[0m=\u001b[32m'END of knowledge_search tool results.\\n'\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this \u001b[0m\n",
       "\u001b[32mquery.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# higher level tool provides packaged results, can span multiple dbs\n",
    "tool_response = client.tool_runtime.rag_tool.query(\n",
    "    content=prompt, vector_db_ids=[vector_db_id]\n",
    ")\n",
    "rich.print(tool_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cac4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryChunksResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chunks</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scores</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryChunksResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchunks\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mscores\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can also query the vector db directly\n",
    "db_response = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=prompt,\n",
    ")\n",
    "rich.print(db_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f07682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_context = tool_response.content\n",
    "prompt_context = \"\\n\".join([c.content for c in db_response.chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97855fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'system'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant.'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and (b) the memory constraints of your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'system'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'You are a helpful assistant.'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your \u001b[0m\n",
       "\u001b[32mhardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to \u001b[0m\n",
       "\u001b[32mexperiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA \u001b[0m\n",
       "\u001b[32mto Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply \u001b[0m\n",
       "\u001b[32mLoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to\u001b[0m\n",
       "\u001b[32mincrease our max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively \u001b[0m\n",
       "\u001b[32mminor.\\n\\nLet\\'s run this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and\u001b[0m\n",
       "\u001b[32mrank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \u001b[0m\n",
       "\u001b[32m--config llama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between \u001b[0m\n",
       "\u001b[32mthis run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "extended_prompt = f\"\"\"\n",
    "Please answer the given query using the context below.\n",
    "\n",
    "QUERY:\n",
    "{prompt}\n",
    "\n",
    "CONTEXT:\n",
    "{prompt_context}\n",
    "\"\"\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "rich.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526a6848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here are the top 5 topics that were explained:\\n\\n* LoRA (Low-Rank Adaptation)\\n* Memory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configurations\\n* Trading off memory and model performance with LoRA'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1592.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1660.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Here are the top 5 topics that were explained:\\n\\n* LoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLow-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Memory \u001b[0m\n",
       "\u001b[32mconstraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA \u001b[0m\n",
       "\u001b[32mconfigurations\\n* Trading off memory and model performance with LoRA'\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1592\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m68\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1660\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=messages,\n",
    "    model_id=\"llama3.2:3b-instruct-fp16\",\n",
    "    timeout=600\n",
    ")\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437e8ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Left as an extra exercise for the reader\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client, \n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools = [\n",
    "        {\n",
    "          \"name\": \"builtin::rag/knowledge_search\",\n",
    "          \"args\" : {\n",
    "            \"vector_db_ids\": [vector_db_id],\n",
    "          }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a80342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/15359f20-907d-496f-ae86-e1da64f9c35f/session \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">User&gt; What is Lora?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "User> What is Lora?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/15359f20-907d-496f-ae86-e1da64f9c35f/session/6dd8de9a-4626-4c93-9079-2de255fca754/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mL\u001b[0m\u001b[33mora\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Lora'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Lora\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m rewritten\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m text\u001b[0m\u001b[33m,\u001b[0m\u001b[33m formatted\u001b[0m\u001b[33m for\u001b[0m\u001b[33m better\u001b[0m\u001b[33m readability\u001b[0m\u001b[33m and\u001b[0m\u001b[33m clarity\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mUsing\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Torch\u001b[0m\u001b[33mT\u001b[0m\u001b[33mune\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m use\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLinear\u001b[0m\u001b[33mly\u001b[0m\u001b[33m Mod\u001b[0m\u001b[33mulated\u001b[0m\u001b[33m Attention\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Torch\u001b[0m\u001b[33mT\u001b[0m\u001b[33mune\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m any\u001b[0m\u001b[33m model\u001b[0m\u001b[33m using\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `_\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m.\u001b[0m\u001b[33m For\u001b[0m\u001b[33m example\u001b[0m\u001b[33m,\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`\u001b[0m\u001b[33m configuration\u001b[0m\u001b[33m,\u001b[0m\u001b[33m run\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m```\u001b[0m\u001b[33mbash\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m run\u001b[0m\u001b[33m l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_f\u001b[0m\u001b[33minet\u001b[0m\u001b[33mune\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m --\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m llama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m``\u001b[0m\u001b[33m`\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mCustom\u001b[0m\u001b[33mizing\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m Parameters\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m customize\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m suit\u001b[0m\u001b[33m your\u001b[0m\u001b[33m needs\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m specify\u001b[0m\u001b[33m two\u001b[0m\u001b[33m sets\u001b[0m\u001b[33m of\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mWhich\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m can\u001b[0m\u001b[33m control\u001b[0m\u001b[33m which\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_at\u001b[0m\u001b[33mtn\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m accepts\u001b[0m\u001b[33m a\u001b[0m\u001b[33m list\u001b[0m\u001b[33m of\u001b[0m\u001b[33m strings\u001b[0m\u001b[33m specifying\u001b[0m\u001b[33m which\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m:\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mq\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mk\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mv\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m value\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33moutput\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m attention\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mApply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m MLP\u001b[0m\u001b[33m and\u001b[0m\u001b[33m output\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m can\u001b[0m\u001b[33m also\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m MLP\u001b[0m\u001b[33m in\u001b[0m\u001b[33m each\u001b[0m\u001b[33m transformer\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_m\u001b[0m\u001b[33mlp\u001b[0m\u001b[33m`,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m's\u001b[0m\u001b[33m final\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_output\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m is\u001b[0m\u001b[33m usually\u001b[0m\u001b[33m a\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m to\u001b[0m\u001b[33m vocabulary\u001b[0m\u001b[33m space\u001b[0m\u001b[33m (\u001b[0m\u001b[33me\u001b[0m\u001b[33m.g\u001b[0m\u001b[33m.,\u001b[0m\u001b[33m in\u001b[0m\u001b[33m language\u001b[0m\u001b[33m models\u001b[0m\u001b[33m),\u001b[0m\u001b[33m but\u001b[0m\u001b[33m other\u001b[0m\u001b[33m modeling\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m may\u001b[0m\u001b[33m have\u001b[0m\u001b[33m different\u001b[0m\u001b[33m projections\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m are\u001b[0m\u001b[33m specified\u001b[0m\u001b[33m under\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mmodel\u001b[0m\u001b[33m`\u001b[0m\u001b[33m flag\u001b[0m\u001b[33m or\u001b[0m\u001b[33m config\u001b[0m\u001b[33m entry\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Note\u001b[0m\u001b[33m that\u001b[0m\u001b[33m models\u001b[0m\u001b[33m using\u001b[0m\u001b[33m tied\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m (\u001b[0m\u001b[33msuch\u001b[0m\u001b[33m as\u001b[0m\u001b[33m Gem\u001b[0m\u001b[33mma\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33mB\u001b[0m\u001b[33m and\u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m final\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m do\u001b[0m\u001b[33m not\u001b[0m\u001b[33m support\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_output\u001b[0m\u001b[33m`.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mGetting\u001b[0m\u001b[33m Started\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m get\u001b[0m\u001b[33m started\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m,\u001b[0m\u001b[33m use\u001b[0m\u001b[33m any\u001b[0m\u001b[33m of\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `_\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m `\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m These\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m utilize\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m-enabled\u001b[0m\u001b[33m model\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m and\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m a\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m set\u001b[0m\u001b[33m of\u001b[0m\u001b[33m configurations\u001b[0m\u001b[33m to\u001b[0m\u001b[33m allow\u001b[0m\u001b[33m you\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m your\u001b[0m\u001b[33m models\u001b[0m\u001b[33m quickly\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "user_prompts = [\n",
    "    \"What is Lora?\"\n",
    "]\n",
    "session_id = rag_agent.create_session(f\"rag session-{uuid.uuid4()}\")\n",
    "for prompt in user_prompts:\n",
    "    rich.print(f\"User> {prompt}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
