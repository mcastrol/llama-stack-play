{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c31c3e8",
   "metadata": {},
   "source": [
    "# RAG from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cdbbf",
   "metadata": {},
   "source": [
    "- Adapted from https://github.com/opendatahub-io/llama-stack-demos/\n",
    "- Requires ollama template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccb0e4",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88da1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"http://localhost:8321\"\n",
    "chunk_size_in_tokens=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "from llama_stack_client import LlamaStackClient, RAGDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8319431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(identifier='llama3.2:3b-instruct-fp16', metadata={}, api_model_type='llm', provider_id='ollama', type='model', provider_resource_id='llama3.2:3b-instruct-fp16', model_type='llm'),\n",
       " Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='ollama', type='model', provider_resource_id='all-minilm:latest', model_type='embedding')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06d2de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient, RAGDocument\n",
    "\n",
    "pdf_urls = [\n",
    "    \"https://arxiv.org/pdf/2304.08641.pdf\",\n",
    "    \"https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"pdf-{i}\",\n",
    "        content=url,  # still use `content` for the URL\n",
    "        mime_type=\"application/pdf\",\n",
    "        metadata={\"source\": \"arxiv\" if \"arxiv\" in url else \"cmu\"},\n",
    "    )\n",
    "    for i, url in enumerate(pdf_urls)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6a69959",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c49a3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document_id': 'num-0',\n",
       "  'content': 'https://www.openshift.guide/openshift-guide-screen.pdf',\n",
       "  'mime_type': 'application/pdf',\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d21e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_stack_client._base_client:Retrying request to /v1/vector-dbs in 0.453451 seconds\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/vector-dbs in 0.781721 seconds\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vector_db_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemoInnovate-vectordb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_dbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/resources/vector_dbs.py:142\u001b[0m, in \u001b[0;36mVectorDBsResource.register\u001b[0;34m(self, embedding_model, vector_db_id, embedding_dimension, provider_id, provider_vector_db_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VectorDBRegisterResponse:\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Register a vector database.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/vector-dbs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_db_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_dimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprovider_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprovider_vector_db_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_vector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvector_db_register_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVectorDBRegisterParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVectorDBRegisterResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1222\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1210\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1219\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1220\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/_base_client.py:999\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1003\u001b[0m     request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1008\u001b[0m )\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "vector_db_id = \"demoInnovate-vectordb\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf72cc",
   "metadata": {},
   "source": [
    "Ingesting documents into a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aaf4f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdf-0'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://arxiv.org/pdf/2304.08641.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'arxiv'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdf-1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cmu'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'pdf-0'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://arxiv.org/pdf/2304.08641.pdf'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'arxiv'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'pdf-1'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'cmu'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2066200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 500 Internal Server Error\"\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/tool-runtime/rag-tool/insert in 0.426489 seconds\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 500 Internal Server Error\"\n",
      "INFO:llama_stack_client._base_client:Retrying request to /v1/tool-runtime/rag-tool/insert in 0.761061 seconds\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size_in_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/resources/tool_runtime/rag_tool.py:75\u001b[0m, in \u001b[0;36mRagToolResource.insert\u001b[0;34m(self, chunk_size_in_tokens, documents, vector_db_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mIndex documents so they can be used by the RAG system\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*/*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/tool-runtime/rag-tool/insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_size_in_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size_in_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_db_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrag_tool_insert_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRagToolInsertParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNoneType\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1222\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1210\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1219\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1220\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/llama-stack-play-3127/lib/python3.10/site-packages/llama_stack_client/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1030\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'detail': 'Internal server error: An unexpected error occurred.'}"
     ]
    }
   ],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c68396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the top 5 topics that were explained? Only list succinct bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2328bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryResult</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span><span style=\"font-weight: bold\">]}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 1\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 2\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 3\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 4\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 5\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'END of knowledge_search tool results.\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 1\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 2\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 3\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 4\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 5\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtext\u001b[0m=\u001b[32m'END of knowledge_search tool results.\\n'\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'The above results were retrieved to help answer the user\\'s query: \"What are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this \u001b[0m\n",
       "\u001b[32mquery.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# higher level tool provides packaged results, can span multiple dbs\n",
    "tool_response = client.tool_runtime.rag_tool.query(\n",
    "    content=prompt, vector_db_ids=[vector_db_id]\n",
    ")\n",
    "rich.print(tool_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cac4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryChunksResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chunks</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scores</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryChunksResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchunks\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mscores\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can also query the vector db directly\n",
    "db_response = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=prompt,\n",
    ")\n",
    "rich.print(db_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f07682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_context = tool_response.content\n",
    "prompt_context = \"\\n\".join([c.content for c in db_response.chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97855fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'system'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant.'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and (b) the memory constraints of your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'system'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'You are a helpful assistant.'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your \u001b[0m\n",
       "\u001b[32mhardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to \u001b[0m\n",
       "\u001b[32mexperiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA \u001b[0m\n",
       "\u001b[32mto Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply \u001b[0m\n",
       "\u001b[32mLoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to\u001b[0m\n",
       "\u001b[32mincrease our max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively \u001b[0m\n",
       "\u001b[32mminor.\\n\\nLet\\'s run this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and\u001b[0m\n",
       "\u001b[32mrank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \u001b[0m\n",
       "\u001b[32m--config llama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between \u001b[0m\n",
       "\u001b[32mthis run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "extended_prompt = f\"\"\"\n",
    "Please answer the given query using the context below.\n",
    "\n",
    "QUERY:\n",
    "{prompt}\n",
    "\n",
    "CONTEXT:\n",
    "{prompt_context}\n",
    "\"\"\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "rich.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526a6848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here are the top 5 topics that were explained:\\n\\n* LoRA (Low-Rank Adaptation)\\n* Memory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configurations\\n* Trading off memory and model performance with LoRA'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1592.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1660.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Here are the top 5 topics that were explained:\\n\\n* LoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLow-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Memory \u001b[0m\n",
       "\u001b[32mconstraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA \u001b[0m\n",
       "\u001b[32mconfigurations\\n* Trading off memory and model performance with LoRA'\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1592\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m68\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1660\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=messages,\n",
    "    model_id=\"llama3.2:3b-instruct-fp16\",\n",
    "    timeout=600\n",
    ")\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437e8ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Left as an extra exercise for the reader\n",
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client, \n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools = [\n",
    "        {\n",
    "          \"name\": \"builtin::rag/knowledge_search\",\n",
    "          \"args\" : {\n",
    "            \"vector_db_ids\": [vector_db_id],\n",
    "          }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a80342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/326578e4-7e72-4068-bbd5-1804814f17a3/session \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">User&gt; What is Lora?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "User> What is Lora?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/326578e4-7e72-4068-bbd5-1804814f17a3/session/f02e6687-c49b-43cc-b489-444f22205294/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mL\u001b[0m\u001b[33mora\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Lora'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Lora\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m rewritten\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m text\u001b[0m\u001b[33m,\u001b[0m\u001b[33m formatted\u001b[0m\u001b[33m for\u001b[0m\u001b[33m better\u001b[0m\u001b[33m readability\u001b[0m\u001b[33m and\u001b[0m\u001b[33m clarity\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mUsing\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Torch\u001b[0m\u001b[33mT\u001b[0m\u001b[33mune\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m use\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLinear\u001b[0m\u001b[33mly\u001b[0m\u001b[33m Mod\u001b[0m\u001b[33mulated\u001b[0m\u001b[33m Attention\u001b[0m\u001b[33m)\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Torch\u001b[0m\u001b[33mT\u001b[0m\u001b[33mune\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m any\u001b[0m\u001b[33m model\u001b[0m\u001b[33m using\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `_\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m.\u001b[0m\u001b[33m For\u001b[0m\u001b[33m example\u001b[0m\u001b[33m,\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`\u001b[0m\u001b[33m configuration\u001b[0m\u001b[33m,\u001b[0m\u001b[33m run\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m```\u001b[0m\u001b[33mbash\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m run\u001b[0m\u001b[33m l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_f\u001b[0m\u001b[33minet\u001b[0m\u001b[33mune\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m --\u001b[0m\u001b[33mconfig\u001b[0m\u001b[33m llama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m``\u001b[0m\u001b[33m`\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mCustom\u001b[0m\u001b[33mizing\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m Parameters\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m customize\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m suit\u001b[0m\u001b[33m your\u001b[0m\u001b[33m needs\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m specify\u001b[0m\u001b[33m two\u001b[0m\u001b[33m sets\u001b[0m\u001b[33m of\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mWhich\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m can\u001b[0m\u001b[33m control\u001b[0m\u001b[33m which\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_at\u001b[0m\u001b[33mtn\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m accepts\u001b[0m\u001b[33m a\u001b[0m\u001b[33m list\u001b[0m\u001b[33m of\u001b[0m\u001b[33m strings\u001b[0m\u001b[33m specifying\u001b[0m\u001b[33m which\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m:\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mq\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mk\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33mv\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m value\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m `\u001b[0m\u001b[33moutput\u001b[0m\u001b[33m_proj\u001b[0m\u001b[33m`\u001b[0m\u001b[33m applies\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m attention\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mApply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m MLP\u001b[0m\u001b[33m and\u001b[0m\u001b[33m output\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m can\u001b[0m\u001b[33m also\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m MLP\u001b[0m\u001b[33m in\u001b[0m\u001b[33m each\u001b[0m\u001b[33m transformer\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_m\u001b[0m\u001b[33mlp\u001b[0m\u001b[33m`,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m apply\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m's\u001b[0m\u001b[33m final\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_output\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m is\u001b[0m\u001b[33m usually\u001b[0m\u001b[33m a\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m to\u001b[0m\u001b[33m vocabulary\u001b[0m\u001b[33m space\u001b[0m\u001b[33m (\u001b[0m\u001b[33me\u001b[0m\u001b[33m.g\u001b[0m\u001b[33m.,\u001b[0m\u001b[33m in\u001b[0m\u001b[33m language\u001b[0m\u001b[33m models\u001b[0m\u001b[33m),\u001b[0m\u001b[33m but\u001b[0m\u001b[33m other\u001b[0m\u001b[33m modeling\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m may\u001b[0m\u001b[33m have\u001b[0m\u001b[33m different\u001b[0m\u001b[33m projections\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m are\u001b[0m\u001b[33m specified\u001b[0m\u001b[33m under\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33mmodel\u001b[0m\u001b[33m`\u001b[0m\u001b[33m flag\u001b[0m\u001b[33m or\u001b[0m\u001b[33m config\u001b[0m\u001b[33m entry\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Note\u001b[0m\u001b[33m that\u001b[0m\u001b[33m models\u001b[0m\u001b[33m using\u001b[0m\u001b[33m tied\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m (\u001b[0m\u001b[33msuch\u001b[0m\u001b[33m as\u001b[0m\u001b[33m Gem\u001b[0m\u001b[33mma\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m \u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33mB\u001b[0m\u001b[33m and\u001b[0m\u001b[33m \u001b[0m\u001b[33m0\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33mB\u001b[0m\u001b[33m)\u001b[0m\u001b[33m for\u001b[0m\u001b[33m the\u001b[0m\u001b[33m final\u001b[0m\u001b[33m output\u001b[0m\u001b[33m projection\u001b[0m\u001b[33m do\u001b[0m\u001b[33m not\u001b[0m\u001b[33m support\u001b[0m\u001b[33m `\u001b[0m\u001b[33mapply\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_output\u001b[0m\u001b[33m`.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mGetting\u001b[0m\u001b[33m Started\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m get\u001b[0m\u001b[33m started\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m,\u001b[0m\u001b[33m use\u001b[0m\u001b[33m any\u001b[0m\u001b[33m of\u001b[0m\u001b[33m our\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `_\u001b[0m\u001b[33ml\u001b[0m\u001b[33mora\u001b[0m\u001b[33m`\u001b[0m\u001b[33m prefix\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m `\u001b[0m\u001b[33mll\u001b[0m\u001b[33mama\u001b[0m\u001b[33m3\u001b[0m\u001b[33m/\u001b[0m\u001b[33m8\u001b[0m\u001b[33mB\u001b[0m\u001b[33m_l\u001b[0m\u001b[33mora\u001b[0m\u001b[33m_single\u001b[0m\u001b[33m_device\u001b[0m\u001b[33m`.\u001b[0m\u001b[33m These\u001b[0m\u001b[33m recipes\u001b[0m\u001b[33m utilize\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m-enabled\u001b[0m\u001b[33m model\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m and\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m a\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m set\u001b[0m\u001b[33m of\u001b[0m\u001b[33m configurations\u001b[0m\u001b[33m to\u001b[0m\u001b[33m allow\u001b[0m\u001b[33m you\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m your\u001b[0m\u001b[33m models\u001b[0m\u001b[33m quickly\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "user_prompts = [\n",
    "    \"What is Lora?\"\n",
    "]\n",
    "session_id = rag_agent.create_session(f\"rag session-{uuid.uuid4()}\")\n",
    "for prompt in user_prompts:\n",
    "    rich.print(f\"User> {prompt}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-play-3127",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
