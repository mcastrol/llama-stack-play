{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c31c3e8",
   "metadata": {},
   "source": [
    "# RAG from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cdbbf",
   "metadata": {},
   "source": [
    "- Adapted from https://github.com/opendatahub-io/llama-stack-demos/\n",
    "- Requires ollama template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccb0e4",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "The variables are configured related to the distribution we are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3136f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "base_url=os.getenv(\"BASE_URL\") #local \"http://localhost:8321\"\n",
    "chunk_size_in_tokens=os.getenv(\"CHUNK_SIZE_TOKEN\") #512\n",
    "collection_name=os.getenv('COLLECTION_NAME')  ##all-MiniLM-L6-v2\"\n",
    "embedding_model_name=os.getenv('EMBEDDING_MODEL_NAME')  \n",
    "embedding_size=int(os.getenv('EMBEDDING_SIZE'))   \n",
    "model_name=os.getenv(\"MODEL_NAME\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28f85e",
   "metadata": {},
   "source": [
    "### Define client and test connection to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8319431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='ollama', type='model', provider_resource_id='all-minilm:latest', model_type='embedding'),\n",
       " Model(identifier='llama3.2:3b-instruct-fp16', metadata={}, api_model_type='llm', provider_id='ollama', type='model', provider_resource_id='llama3.2:3b-instruct-fp16', model_type='llm')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222fd1c",
   "metadata": {},
   "source": [
    "### Ingest data to vector db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06d2de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import RAGDocument\n",
    "\n",
    "#urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
    "urls = [\"chat.rst\", \"llama3.rst\"]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, url in enumerate(urls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d21e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='demoInnovate-vectordb', provider_id='faiss', type='vector_db', provider_resource_id='demoInnovate-vectordb', owner={'principal': '', 'attributes': {}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db_id = collection_name\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=embedding_model_name,\n",
    "    embedding_dimension=embedding_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf72cc",
   "metadata": {},
   "source": [
    "Ingesting documents into a vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaf4f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-2'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'mime_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-0'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-1'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-2'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst'\u001b[0m,\n",
       "        \u001b[32m'mime_type'\u001b[0m: \u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich\n",
    "rich.print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2066200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=chunk_size_in_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ca5df",
   "metadata": {},
   "source": [
    "## Testing vector db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the top 5 topics that were explained? Only list succinct bullet points.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a5f60",
   "metadata": {},
   "source": [
    "### Using tool_runtime api that can query in multiple collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2328bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryResult</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span><span style=\"font-weight: bold\">]}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 1\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">templates &amp; special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time?\",\\n        },\\n        {\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">},\\n    ]\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate, Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently\\nMetadata: {\\'document_id\\': \\'num-0\\'}\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 2\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">templates &amp; special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time?\",\\n        },\\n        {\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">},\\n    ]\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate, Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently\\nMetadata: {\\'document_id\\': \\'num-0\\'}\\n'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">text</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Result 3\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">templates &amp; special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = [\\n        {\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        },\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time?\",\\n        },\\n        {\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">},\\n    ]\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate, Message\\n\\n    messages = [Message.from_dict(msg) for msg in sample]\\n    formatted_messages = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2ChatTemplate.format(messages)\\n    print(formatted_messages)\\n    # [\\n    #     Message(\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">role=\\'user\\',\\n    #         content=\\'[INST] &lt;&lt;SYS&gt;&gt;\\\\nYou are a helpful, respectful, and honest </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assistant.\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\nWho are the most influential hip-hop artists of all time? [/INST] \\',\\n    #         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    #     Message(\\n    #         role=\\'assistant\\',\\n    #         content=\\'Here is a list </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">...,\\n    #     ),\\n    # ]\\n\\nThere are also special tokens used by Llama2, which are not in the prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">template.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don\\'t include the :code:`&lt;s&gt;` and :code:`&lt;/s&gt;` tokens. These are the beginning-of-sequence\\n(BOS) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">end-of-sequence (EOS) tokens that are represented differently\\nMetadata: {\\'document_id\\': \\'num-0\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 4\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Result 5\\nContent: ,\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment. We can also increase alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\nMetadata: {\\'document_id\\': \\'num-3\\'}\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'END of knowledge_search tool results.\\n'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextContentItem</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The above results were retrieved to help answer the user\\'s query: \"What are the top 3 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.\\n'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'num-0'\u001b[0m, \u001b[32m'num-0'\u001b[0m, \u001b[32m'num-0'\u001b[0m, \u001b[32m'num-3'\u001b[0m, \u001b[32m'num-3'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 1\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt \u001b[0m\n",
       "\u001b[32mtemplates & special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single \u001b[0m\n",
       "\u001b[32muser-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32m\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all \u001b[0m\n",
       "\u001b[32mtime?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of\u001b[0m\n",
       "\u001b[32mthe most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    \u001b[0m\n",
       "\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee \u001b[0m\n",
       "\u001b[32mhow it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a \u001b[0m\n",
       "\u001b[32mprompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate, Message\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m<\u001b[0m\u001b[32m<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-0\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 2\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt \u001b[0m\n",
       "\u001b[32mtemplates & special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single \u001b[0m\n",
       "\u001b[32muser-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32m\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all \u001b[0m\n",
       "\u001b[32mtime?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of\u001b[0m\n",
       "\u001b[32mthe most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    \u001b[0m\n",
       "\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee \u001b[0m\n",
       "\u001b[32mhow it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a \u001b[0m\n",
       "\u001b[32mprompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate, Message\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m <<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s>` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-0\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text'\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mTextContentItem\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Result 3\\nContent:  Instruct.\\n\\n.. _prompt_template_vs_special_tokens:\\n\\nTokenizing prompt \u001b[0m\n",
       "\u001b[32mtemplates & special tokens\\n--------------------------------------------\\n\\nLet\\'s say I have a sample of a single \u001b[0m\n",
       "\u001b[32muser-assistant turn accompanied with a system\\nprompt:\\n\\n.. code-block:: python\\n\\n    sample = \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32m\"role\": \"system\",\\n            \"content\": \"You are a helpful, respectful, and honest assistant.\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"user\",\\n            \"content\": \"Who are the most influential hip-hop artists of all \u001b[0m\n",
       "\u001b[32mtime?\",\\n        \u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\n        \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n            \"role\": \"assistant\",\\n            \"content\": \"Here is a list of some of\u001b[0m\n",
       "\u001b[32mthe most influential hip-hop \"\\n            \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\\n    \u001b[0m\n",
       "\u001b[32m}\u001b[0m\u001b[32m,\\n    \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nNow, let\\'s format this with the :class:`~torchtune.models.llama2.Llama2ChatTemplate` class and\\nsee \u001b[0m\n",
       "\u001b[32mhow it gets tokenized. The Llama2ChatTemplate is an example of a **prompt template**,\\nwhich simply structures a \u001b[0m\n",
       "\u001b[32mprompt with flavor text to indicate a certain task.\\n\\n.. code-block:: python\\n\\n    from torchtune.data import \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate, Message\\n\\n    messages = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessage.from_dict\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmsg\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for msg in sample\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    formatted_messages = \u001b[0m\n",
       "\u001b[32mLlama2ChatTemplate.format\u001b[0m\u001b[32m(\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    print\u001b[0m\u001b[32m(\u001b[0m\u001b[32mformatted_messages\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    # \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\n",
       "\u001b[32mrole\u001b[0m\u001b[32m=\\'user\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mINST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m <<SYS>>\\\\nYou are a helpful, respectful, and honest \u001b[0m\n",
       "\u001b[32massistant.\\\\n<</SYS>>\\\\n\\\\nWho are the most influential hip-hop artists of all time? \u001b[0m\u001b[32m[\u001b[0m\u001b[32m/INST\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\',\\n    #         \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    #     Message\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n    #         \u001b[0m\u001b[32mrole\u001b[0m\u001b[32m=\\'assistant\\',\\n    #         \u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m=\\'Here is a list \u001b[0m\n",
       "\u001b[32mof some of the most influential hip-hop artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\\',\\n    #   \u001b[0m\n",
       "\u001b[32m...,\\n    #     \u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n    # \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nThere are also special tokens used by Llama2, which are not in the prompt \u001b[0m\n",
       "\u001b[32mtemplate.\\nIf you look at our :class:`~torchtune.models.llama2.Llama2ChatTemplate` class, you\\'ll notice that\\nwe \u001b[0m\n",
       "\u001b[32mdon\\'t include the :code:`<s>` and :code:`</s\u001b[0m\u001b[32m>\u001b[0m\u001b[32m` tokens. These are the beginning-of-sequence\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mend-of-sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEOS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tokens that are represented differently\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-0\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 4\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'Result 5\\nContent: ,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding \u001b[0m\n",
       "\u001b[32mcommand will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s \u001b[0m\n",
       "\u001b[32mtake a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model \u001b[0m\n",
       "\u001b[32mArguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    \u001b[0m\n",
       "\u001b[32mlora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a \u001b[0m\n",
       "\u001b[32mrank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all linear layers \u001b[0m\n",
       "\u001b[32min\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase our max \u001b[0m\n",
       "\u001b[32mmemory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this \u001b[0m\n",
       "\u001b[32mexperiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \u001b[0m\n",
       "\u001b[32m\\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\nMetadata: \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'document_id\\': \\'num-3\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtext\u001b[0m=\u001b[32m'END of knowledge_search tool results.\\n'\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTextContentItem\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtext\u001b[0m=\u001b[32m'The above results were retrieved to help answer the user\\'s query: \"What are the top 3 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\". Use them as supporting information only in answering this \u001b[0m\n",
       "\u001b[32mquery.\\n'\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# higher level tool provides packaged results, can span multiple dbs\n",
    "tool_response = client.tool_runtime.rag_tool.query(\n",
    "    content=prompt, vector_db_ids=[vector_db_id]\n",
    ")\n",
    "rich.print(tool_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fb3cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method query in module llama_stack_client.resources.tool_runtime.rag_tool:\n",
      "\n",
      "query(*, content: 'InterleavedContent', vector_db_ids: 'List[str]', query_config: 'QueryConfig | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'QueryResult' method of llama_stack_client.resources.tool_runtime.rag_tool.RagToolResource instance\n",
      "    Query the RAG system for context; typically invoked by the agent\n",
      "    \n",
      "    Args:\n",
      "      content: A image content item\n",
      "    \n",
      "      query_config: Configuration for the RAG query generation.\n",
      "    \n",
      "      extra_headers: Send extra headers\n",
      "    \n",
      "      extra_query: Add additional query parameters to the request\n",
      "    \n",
      "      extra_body: Add additional JSON properties to the request\n",
      "    \n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.tool_runtime.rag_tool.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d85af",
   "metadata": {},
   "source": [
    "### Query the vector io collection directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cac4316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/vector-io/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryChunksResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chunks</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00406104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048808675</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.054821957</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05163342</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009840118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09178936</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0101559</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04716209</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02568053</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014743081</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023843573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03369005</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018293295</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022487223</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010905134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07937251</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.108631425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018547243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056368362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012521768</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06545227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07763688</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10424298</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017161276</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06960253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00437907</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008911352</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0038266839</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039670363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12211394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06891109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06289771</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07961002</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04354608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07493716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008346766</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05844109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034176067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06328286</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031867906</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.066620894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.060622793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068750225</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014350068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09729832</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034628067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023585148</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06224204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023213899</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030940313</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05188855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009149888</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03583717</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045438588</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09836591</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053864855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03148368</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.006111429</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021325532</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06396347</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037958525</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.015609006</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06672296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012821037</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07735162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06136182</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025389215</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1137226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050520107</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026594693</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023067558</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046904422</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.050398633</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0881821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0050534597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010243519</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053621396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.080357976</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04746425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03897748</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022939207</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03886172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01014672</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03948978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005702496</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0053803157</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03578825</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010108946</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05629815</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07863444</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04478068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07231891</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.088722266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01428813</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.02099274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.105825774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0030984902</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00082296424</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.111134134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.065025434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030771716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08274354</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029646</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0027542152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11878068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054552376</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09019618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08030197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045539204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032676317</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010045935</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00045448847</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.057014752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010716465</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06508274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08829183</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046921328</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0941425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.033974033</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.071631305</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.043230794</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018983835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056854513</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023464497</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12349351</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41047145e-33</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09405394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.053460535</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.057672363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021022841</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0437118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0036255645</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05365767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0017974502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11173805</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021721944</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020497402</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03737283</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02771337</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055384185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030681066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014651615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03798954</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014513726</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08858634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043590903</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054311052</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01135104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002714408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015660262</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06580455</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016221873</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008818226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05178188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.10335743</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0061823605</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05804722</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0048823925</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011941615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007956622</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016165752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008485277</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017111996</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058726076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009807942</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09311245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021275343</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09670583</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032374088</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048536807</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015776193</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008340045</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058264416</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10344821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05076908</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017378442</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03017162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01780222</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07665155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046623345</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011975792</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022626506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.037155885</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03966967</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045327567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010550249</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046029713</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002898502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020544779</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.028050574</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009646118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014328015</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010246864</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002703339</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04329241</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0681562</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009135491</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0671044</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.051254403</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0380243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031048004</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08539608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023425952</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.059093464</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03771963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005505408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08227611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0027852387</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02822454</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03574911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.088758364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08574517</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06668714</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04406618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06357254</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09611272</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055917</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08705364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.095189266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03884191</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.099543266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.4062036e-34</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019374205</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008741152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024491386</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04814948</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025727019</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041861188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0324116</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022669611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.072869636</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046576407</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00093093707</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.049277034</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0024528275</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03657786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07719197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.045579806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029033057</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035945218</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08609477</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05367174</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03351355</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038600307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10361258</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027757617</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06351757</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035390947</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0014317943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006192092</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.039058138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11213894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046052806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04218518</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06636131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032723434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026828311</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027669417</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054873694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730395</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035801467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010242396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002786516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004298824</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068057775</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036689095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024299202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010829978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034187958</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014378679</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010835362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08472067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021373762</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017250067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042057704</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026164098</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032092307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025745966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05531999</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045077786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01748674</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06220995</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.035986185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0434911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05527629</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08849172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039227657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020592552</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.004950738</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03919372</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0054969187</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015540091</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019606767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717856</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.038650155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010577874</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07570042</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.027161708</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018637853</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009169965</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05081357</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005626072</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0880178</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009923634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.003280131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04810389</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030830374</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06312132</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.102168076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.062001597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03150046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032853913</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009290752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.011255245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005275253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0006773811</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024117803</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.285374e-08</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.092341095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07954865</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043613728</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07328332</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0034253467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07900413</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061924793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.100435086</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0069704335</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.040756296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06753555</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018935567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026013983</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0011952032</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13679461</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026989138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050523963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061521284</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005472806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041764557</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.069850735</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06550046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0014655694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004496941</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00613516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021994136</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020913554</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061921928</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.049571835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023379657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07368106</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0025017066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038181227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07607061</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03182009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04869202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.028599966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.034752943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027080975</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032496396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0036284167</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04125114</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042432863</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046958406</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008400537</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026365466</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07602573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07147453</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03766765</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08318256</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036861774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0061818687</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021962399</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05041582</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05621483</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037829414</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09600329</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017125007</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03444506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032913096</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.099935316</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017872145</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.096478194</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0055429</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00406104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048808675</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.054821957</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05163342</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009840118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09178936</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0101559</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04716209</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02568053</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014743081</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023843573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03369005</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018293295</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022487223</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010905134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07937251</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.108631425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018547243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056368362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012521768</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06545227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07763688</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10424298</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017161276</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06960253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00437907</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008911352</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0038266839</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039670363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12211394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06891109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06289771</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07961002</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04354608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07493716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008346766</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05844109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034176067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06328286</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031867906</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.066620894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.060622793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068750225</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014350068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09729832</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034628067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023585148</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06224204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023213899</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030940313</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05188855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009149888</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03583717</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045438588</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09836591</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053864855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03148368</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.006111429</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021325532</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06396347</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037958525</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.015609006</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06672296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012821037</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07735162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06136182</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025389215</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1137226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050520107</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026594693</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023067558</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046904422</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.050398633</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0881821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0050534597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010243519</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053621396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.080357976</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04746425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03897748</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022939207</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03886172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01014672</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03948978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005702496</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0053803157</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03578825</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010108946</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05629815</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07863444</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04478068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07231891</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.088722266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01428813</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.02099274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.105825774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0030984902</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00082296424</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.111134134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.065025434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030771716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08274354</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029646</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0027542152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11878068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054552376</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09019618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08030197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045539204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032676317</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010045935</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00045448847</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.057014752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010716465</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06508274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08829183</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046921328</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0941425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.033974033</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.071631305</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.043230794</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018983835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056854513</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023464497</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12349351</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41047145e-33</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09405394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.053460535</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.057672363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021022841</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0437118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0036255645</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05365767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0017974502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11173805</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021721944</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020497402</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03737283</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02771337</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055384185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030681066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014651615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03798954</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014513726</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08858634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043590903</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054311052</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01135104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002714408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015660262</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06580455</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016221873</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008818226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05178188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.10335743</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0061823605</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05804722</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0048823925</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011941615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007956622</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016165752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008485277</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017111996</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058726076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009807942</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09311245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021275343</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09670583</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032374088</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048536807</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015776193</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008340045</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058264416</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10344821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05076908</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017378442</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03017162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01780222</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07665155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046623345</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011975792</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022626506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.037155885</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03966967</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045327567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010550249</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046029713</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002898502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020544779</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.028050574</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009646118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014328015</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010246864</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002703339</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04329241</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0681562</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009135491</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0671044</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.051254403</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0380243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031048004</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08539608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023425952</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.059093464</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03771963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005505408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08227611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0027852387</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02822454</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03574911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.088758364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08574517</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06668714</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04406618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06357254</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09611272</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055917</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08705364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.095189266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03884191</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.099543266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.4062036e-34</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019374205</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008741152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024491386</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04814948</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025727019</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041861188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0324116</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022669611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.072869636</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046576407</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00093093707</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.049277034</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0024528275</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03657786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07719197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.045579806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029033057</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035945218</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08609477</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05367174</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03351355</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038600307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10361258</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027757617</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06351757</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035390947</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0014317943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006192092</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.039058138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11213894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046052806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04218518</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06636131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032723434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026828311</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027669417</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054873694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730395</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035801467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010242396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002786516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004298824</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068057775</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036689095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024299202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010829978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034187958</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014378679</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010835362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08472067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021373762</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017250067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042057704</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026164098</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032092307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025745966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05531999</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045077786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01748674</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06220995</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.035986185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0434911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05527629</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08849172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039227657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020592552</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.004950738</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03919372</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0054969187</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015540091</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019606767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717856</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.038650155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010577874</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07570042</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.027161708</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018637853</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009169965</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05081357</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005626072</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0880178</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009923634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.003280131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04810389</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030830374</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06312132</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.102168076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.062001597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03150046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032853913</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009290752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.011255245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005275253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0006773811</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024117803</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.285374e-08</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.092341095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07954865</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043613728</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07328332</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0034253467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07900413</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061924793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.100435086</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0069704335</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.040756296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06753555</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018935567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026013983</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0011952032</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13679461</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026989138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050523963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061521284</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005472806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041764557</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.069850735</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06550046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0014655694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004496941</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00613516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021994136</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020913554</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061921928</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.049571835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023379657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07368106</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0025017066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038181227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07607061</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03182009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04869202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.028599966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.034752943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027080975</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032496396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0036284167</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04125114</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042432863</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046958406</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008400537</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026365466</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07602573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07147453</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03766765</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08318256</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036861774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0061818687</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021962399</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05041582</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05621483</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037829414</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09600329</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017125007</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03444506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032913096</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.099935316</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017872145</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.096478194</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0055429</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Chunk</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">',\\n    and (b) the memory constraints of your hardware.\\n\\nThe preceding command will run a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alpha (in general it is good practice to scale alpha and rank together).\\n\\n.. code-block:: bash\\n\\n    tune run </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 lora_alpha=64 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our baseline over</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The above figure was generated with W&amp;B. You can use torchtune\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'document_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'num-3'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00406104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048808675</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.054821957</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05163342</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009840118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09178936</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0101559</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04716209</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02568053</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014743081</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023843573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03369005</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018293295</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022487223</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010905134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07937251</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.108631425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018547243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056368362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012521768</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06545227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07763688</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10424298</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017161276</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06960253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00437907</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008911352</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0038266839</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039670363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12211394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06891109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06289771</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07961002</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04354608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07493716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008346766</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05844109</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034176067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06328286</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031867906</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.066620894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.060622793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068750225</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014350068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09729832</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034628067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023585148</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06224204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023213899</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030940313</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05188855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009149888</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03583717</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045438588</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09836591</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053864855</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03148368</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.006111429</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021325532</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06396347</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037958525</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.015609006</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06672296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012821037</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07735162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06136182</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025389215</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1137226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050520107</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026594693</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023067558</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046904422</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.050398633</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0881821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0050534597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010243519</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.053621396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.080357976</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04746425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03897748</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022939207</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03886172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01014672</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03948978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005702496</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0053803157</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03578825</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010108946</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05629815</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07863444</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04478068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07231891</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.088722266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01428813</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.02099274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.105825774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0030984902</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00082296424</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.111134134</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.065025434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.030771716</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08274354</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029646</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0027542152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11878068</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054552376</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09019618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08030197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045539204</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032676317</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010045935</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00045448847</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.057014752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010716465</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06508274</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08829183</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046921328</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0941425</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.033974033</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.071631305</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.043230794</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018983835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.056854513</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023464497</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.12349351</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41047145e-33</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09405394</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.053460535</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.057672363</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021022841</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0437118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0036255645</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05365767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0017974502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11173805</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021721944</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020497402</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03737283</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02771337</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055384185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030681066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014651615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03798954</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014513726</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08858634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043590903</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054311052</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01135104</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002714408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015660262</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06580455</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016221873</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008818226</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05178188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.10335743</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0061823605</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05804722</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0048823925</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011941615</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007956622</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016165752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008485277</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017111996</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058726076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009807942</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09311245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.021275343</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09670583</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032374088</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.048536807</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015776193</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008340045</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.058264416</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10344821</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05076908</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017378442</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03017162</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01780222</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07665155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046623345</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011975792</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.022626506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.037155885</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03966967</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045327567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010550249</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046029713</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.002898502</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020544779</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.028050574</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009646118</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014328015</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010246864</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002703339</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04329241</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0681562</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009135491</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0671044</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.051254403</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0380243</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031048004</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08539608</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.023425952</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.059093464</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03771963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005505408</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08227611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0027852387</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02822454</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03574911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.088758364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08574517</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06668714</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04406618</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06357254</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09611272</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.055917</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08705364</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.095189266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03884191</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.099543266</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-6.4062036e-34</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019374205</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008741152</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024491386</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04814948</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025727019</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041861188</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0324116</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.022669611</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.072869636</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046576407</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00093093707</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.049277034</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0024528275</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03657786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07719197</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.045579806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.029033057</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035945218</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08609477</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05367174</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03351355</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038600307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10361258</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027757617</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06351757</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035390947</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0014317943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.006192092</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.039058138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.11213894</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.046052806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04218518</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06636131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032723434</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026828311</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027669417</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054873694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06730395</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.035801467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.010242396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.002786516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004298824</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.068057775</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036689095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024299202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010829978</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.034187958</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.014378679</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010835362</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08472067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021373762</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017250067</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042057704</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026164098</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032092307</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.025745966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05531999</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.045077786</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01748674</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.06220995</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.035986185</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0434911</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.05527629</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08849172</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039227657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020592552</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.004950738</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03919372</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0054969187</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.015540091</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019606767</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023717856</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.038650155</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010577874</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07570042</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.027161708</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.018637853</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009169965</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05081357</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.005626072</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0880178</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.009923634</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.003280131</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04810389</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.030830374</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06312132</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.102168076</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.062001597</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03150046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032853913</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009290752</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.011255245</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005275253</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0006773811</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.024117803</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-4.285374e-08</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.092341095</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07954865</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.043613728</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07328332</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0034253467</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07900413</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061924793</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.100435086</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0069704335</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.040756296</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06753555</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018935567</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026013983</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0011952032</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13679461</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.026989138</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.050523963</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061521284</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.005472806</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.041764557</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.069850735</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06550046</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0014655694</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004496941</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.00613516</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021994136</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.020913554</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061921928</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.049571835</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.023379657</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07368106</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0025017066</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.038181227</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07607061</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.03182009</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04869202</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.028599966</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.034752943</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027080975</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032496396</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0036284167</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.04125114</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042432863</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046958406</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.008400537</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026365466</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07602573</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.07147453</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03766765</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08318256</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036861774</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0061818687</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.021962399</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05041582</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05621483</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.037829414</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09600329</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.017125007</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03444506</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.032913096</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.099935316</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017872145</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.096478194</span>,\n",
       "                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0055429</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scores</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5663027820679463</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryChunksResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchunks\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;36m0.00406104\u001b[0m,\n",
       "                \u001b[1;36m-0.048808675\u001b[0m,\n",
       "                \u001b[1;36m-0.054821957\u001b[0m,\n",
       "                \u001b[1;36m-0.05163342\u001b[0m,\n",
       "                \u001b[1;36m0.009840118\u001b[0m,\n",
       "                \u001b[1;36m0.09178936\u001b[0m,\n",
       "                \u001b[1;36m-0.0101559\u001b[0m,\n",
       "                \u001b[1;36m0.04716209\u001b[0m,\n",
       "                \u001b[1;36m0.02568053\u001b[0m,\n",
       "                \u001b[1;36m-0.014743081\u001b[0m,\n",
       "                \u001b[1;36m-0.023843573\u001b[0m,\n",
       "                \u001b[1;36m0.03369005\u001b[0m,\n",
       "                \u001b[1;36m-0.018293295\u001b[0m,\n",
       "                \u001b[1;36m0.022487223\u001b[0m,\n",
       "                \u001b[1;36m-0.010905134\u001b[0m,\n",
       "                \u001b[1;36m0.07937251\u001b[0m,\n",
       "                \u001b[1;36m0.108631425\u001b[0m,\n",
       "                \u001b[1;36m0.018547243\u001b[0m,\n",
       "                \u001b[1;36m-0.056368362\u001b[0m,\n",
       "                \u001b[1;36m0.012521768\u001b[0m,\n",
       "                \u001b[1;36m0.06545227\u001b[0m,\n",
       "                \u001b[1;36m-0.07763688\u001b[0m,\n",
       "                \u001b[1;36m0.10424298\u001b[0m,\n",
       "                \u001b[1;36m0.017161276\u001b[0m,\n",
       "                \u001b[1;36m0.06960253\u001b[0m,\n",
       "                \u001b[1;36m-0.00437907\u001b[0m,\n",
       "                \u001b[1;36m-0.008911352\u001b[0m,\n",
       "                \u001b[1;36m-0.0038266839\u001b[0m,\n",
       "                \u001b[1;36m0.039670363\u001b[0m,\n",
       "                \u001b[1;36m-0.12211394\u001b[0m,\n",
       "                \u001b[1;36m0.06891109\u001b[0m,\n",
       "                \u001b[1;36m0.06289771\u001b[0m,\n",
       "                \u001b[1;36m0.07961002\u001b[0m,\n",
       "                \u001b[1;36m0.04354608\u001b[0m,\n",
       "                \u001b[1;36m-0.07493716\u001b[0m,\n",
       "                \u001b[1;36m0.008346766\u001b[0m,\n",
       "                \u001b[1;36m-0.05844109\u001b[0m,\n",
       "                \u001b[1;36m-0.034176067\u001b[0m,\n",
       "                \u001b[1;36m-0.06328286\u001b[0m,\n",
       "                \u001b[1;36m0.031867906\u001b[0m,\n",
       "                \u001b[1;36m0.066620894\u001b[0m,\n",
       "                \u001b[1;36m0.060622793\u001b[0m,\n",
       "                \u001b[1;36m-0.068750225\u001b[0m,\n",
       "                \u001b[1;36m0.014350068\u001b[0m,\n",
       "                \u001b[1;36m0.09729832\u001b[0m,\n",
       "                \u001b[1;36m-0.034628067\u001b[0m,\n",
       "                \u001b[1;36m0.023585148\u001b[0m,\n",
       "                \u001b[1;36m-0.06224204\u001b[0m,\n",
       "                \u001b[1;36m-0.023213899\u001b[0m,\n",
       "                \u001b[1;36m-0.030940313\u001b[0m,\n",
       "                \u001b[1;36m-0.05188855\u001b[0m,\n",
       "                \u001b[1;36m-0.009149888\u001b[0m,\n",
       "                \u001b[1;36m-0.03583717\u001b[0m,\n",
       "                \u001b[1;36m0.045438588\u001b[0m,\n",
       "                \u001b[1;36m-0.09836591\u001b[0m,\n",
       "                \u001b[1;36m-0.053864855\u001b[0m,\n",
       "                \u001b[1;36m-0.03148368\u001b[0m,\n",
       "                \u001b[1;36m-0.006111429\u001b[0m,\n",
       "                \u001b[1;36m0.021325532\u001b[0m,\n",
       "                \u001b[1;36m0.06396347\u001b[0m,\n",
       "                \u001b[1;36m-0.037958525\u001b[0m,\n",
       "                \u001b[1;36m0.015609006\u001b[0m,\n",
       "                \u001b[1;36m0.06672296\u001b[0m,\n",
       "                \u001b[1;36m0.012821037\u001b[0m,\n",
       "                \u001b[1;36m0.07735162\u001b[0m,\n",
       "                \u001b[1;36m-0.06730516\u001b[0m,\n",
       "                \u001b[1;36m0.06136182\u001b[0m,\n",
       "                \u001b[1;36m-0.025389215\u001b[0m,\n",
       "                \u001b[1;36m-0.1137226\u001b[0m,\n",
       "                \u001b[1;36m0.050520107\u001b[0m,\n",
       "                \u001b[1;36m-0.026594693\u001b[0m,\n",
       "                \u001b[1;36m-0.023067558\u001b[0m,\n",
       "                \u001b[1;36m-0.046904422\u001b[0m,\n",
       "                \u001b[1;36m-0.050398633\u001b[0m,\n",
       "                \u001b[1;36m0.0881821\u001b[0m,\n",
       "                \u001b[1;36m0.0050534597\u001b[0m,\n",
       "                \u001b[1;36m0.010243519\u001b[0m,\n",
       "                \u001b[1;36m-0.053621396\u001b[0m,\n",
       "                \u001b[1;36m0.080357976\u001b[0m,\n",
       "                \u001b[1;36m-0.04746425\u001b[0m,\n",
       "                \u001b[1;36m0.03897748\u001b[0m,\n",
       "                \u001b[1;36m-0.022939207\u001b[0m,\n",
       "                \u001b[1;36m-0.03886172\u001b[0m,\n",
       "                \u001b[1;36m0.01014672\u001b[0m,\n",
       "                \u001b[1;36m0.03948978\u001b[0m,\n",
       "                \u001b[1;36m0.005702496\u001b[0m,\n",
       "                \u001b[1;36m0.0053803157\u001b[0m,\n",
       "                \u001b[1;36m-0.03578825\u001b[0m,\n",
       "                \u001b[1;36m-0.010108946\u001b[0m,\n",
       "                \u001b[1;36m-0.023717009\u001b[0m,\n",
       "                \u001b[1;36m0.05629815\u001b[0m,\n",
       "                \u001b[1;36m-0.07863444\u001b[0m,\n",
       "                \u001b[1;36m-0.04478068\u001b[0m,\n",
       "                \u001b[1;36m0.07231891\u001b[0m,\n",
       "                \u001b[1;36m0.088722266\u001b[0m,\n",
       "                \u001b[1;36m-0.01428813\u001b[0m,\n",
       "                \u001b[1;36m-0.02099274\u001b[0m,\n",
       "                \u001b[1;36m-0.105825774\u001b[0m,\n",
       "                \u001b[1;36m-0.0030984902\u001b[0m,\n",
       "                \u001b[1;36m0.00082296424\u001b[0m,\n",
       "                \u001b[1;36m0.111134134\u001b[0m,\n",
       "                \u001b[1;36m-0.065025434\u001b[0m,\n",
       "                \u001b[1;36m-0.030771716\u001b[0m,\n",
       "                \u001b[1;36m0.08274354\u001b[0m,\n",
       "                \u001b[1;36m-0.029646\u001b[0m,\n",
       "                \u001b[1;36m-0.0027542152\u001b[0m,\n",
       "                \u001b[1;36m0.11878068\u001b[0m,\n",
       "                \u001b[1;36m0.054552376\u001b[0m,\n",
       "                \u001b[1;36m-0.09019618\u001b[0m,\n",
       "                \u001b[1;36m-0.08030197\u001b[0m,\n",
       "                \u001b[1;36m0.045539204\u001b[0m,\n",
       "                \u001b[1;36m0.032676317\u001b[0m,\n",
       "                \u001b[1;36m0.010045935\u001b[0m,\n",
       "                \u001b[1;36m0.00045448847\u001b[0m,\n",
       "                \u001b[1;36m0.057014752\u001b[0m,\n",
       "                \u001b[1;36m-0.010716465\u001b[0m,\n",
       "                \u001b[1;36m0.06508274\u001b[0m,\n",
       "                \u001b[1;36m0.08829183\u001b[0m,\n",
       "                \u001b[1;36m0.046921328\u001b[0m,\n",
       "                \u001b[1;36m-0.0941425\u001b[0m,\n",
       "                \u001b[1;36m-0.033974033\u001b[0m,\n",
       "                \u001b[1;36m-0.071631305\u001b[0m,\n",
       "                \u001b[1;36m0.043230794\u001b[0m,\n",
       "                \u001b[1;36m0.018983835\u001b[0m,\n",
       "                \u001b[1;36m-0.056854513\u001b[0m,\n",
       "                \u001b[1;36m-0.023464497\u001b[0m,\n",
       "                \u001b[1;36m-0.12349351\u001b[0m,\n",
       "                \u001b[1;36m1.41047145e-33\u001b[0m,\n",
       "                \u001b[1;36m0.09405394\u001b[0m,\n",
       "                \u001b[1;36m0.053460535\u001b[0m,\n",
       "                \u001b[1;36m-0.057672363\u001b[0m,\n",
       "                \u001b[1;36m-0.021022841\u001b[0m,\n",
       "                \u001b[1;36m-0.0437118\u001b[0m,\n",
       "                \u001b[1;36m0.0036255645\u001b[0m,\n",
       "                \u001b[1;36m0.05365767\u001b[0m,\n",
       "                \u001b[1;36m0.0017974502\u001b[0m,\n",
       "                \u001b[1;36m-0.11173805\u001b[0m,\n",
       "                \u001b[1;36m-0.021721944\u001b[0m,\n",
       "                \u001b[1;36m-0.020497402\u001b[0m,\n",
       "                \u001b[1;36m-0.03737283\u001b[0m,\n",
       "                \u001b[1;36m0.02771337\u001b[0m,\n",
       "                \u001b[1;36m0.055384185\u001b[0m,\n",
       "                \u001b[1;36m0.030681066\u001b[0m,\n",
       "                \u001b[1;36m0.014651615\u001b[0m,\n",
       "                \u001b[1;36m0.03798954\u001b[0m,\n",
       "                \u001b[1;36m0.014513726\u001b[0m,\n",
       "                \u001b[1;36m-0.08858634\u001b[0m,\n",
       "                \u001b[1;36m-0.043590903\u001b[0m,\n",
       "                \u001b[1;36m0.054311052\u001b[0m,\n",
       "                \u001b[1;36m0.01135104\u001b[0m,\n",
       "                \u001b[1;36m-0.002714408\u001b[0m,\n",
       "                \u001b[1;36m-0.015660262\u001b[0m,\n",
       "                \u001b[1;36m0.06580455\u001b[0m,\n",
       "                \u001b[1;36m0.016221873\u001b[0m,\n",
       "                \u001b[1;36m0.008818226\u001b[0m,\n",
       "                \u001b[1;36m-0.05178188\u001b[0m,\n",
       "                \u001b[1;36m-0.10335743\u001b[0m,\n",
       "                \u001b[1;36m0.0061823605\u001b[0m,\n",
       "                \u001b[1;36m-0.05804722\u001b[0m,\n",
       "                \u001b[1;36m0.0048823925\u001b[0m,\n",
       "                \u001b[1;36m0.011941615\u001b[0m,\n",
       "                \u001b[1;36m0.007956622\u001b[0m,\n",
       "                \u001b[1;36m0.016165752\u001b[0m,\n",
       "                \u001b[1;36m-0.008485277\u001b[0m,\n",
       "                \u001b[1;36m-0.017111996\u001b[0m,\n",
       "                \u001b[1;36m0.058726076\u001b[0m,\n",
       "                \u001b[1;36m0.009807942\u001b[0m,\n",
       "                \u001b[1;36m-0.09311245\u001b[0m,\n",
       "                \u001b[1;36m0.021275343\u001b[0m,\n",
       "                \u001b[1;36m0.09670583\u001b[0m,\n",
       "                \u001b[1;36m0.032374088\u001b[0m,\n",
       "                \u001b[1;36m-0.048536807\u001b[0m,\n",
       "                \u001b[1;36m-0.015776193\u001b[0m,\n",
       "                \u001b[1;36m0.008340045\u001b[0m,\n",
       "                \u001b[1;36m0.058264416\u001b[0m,\n",
       "                \u001b[1;36m0.10344821\u001b[0m,\n",
       "                \u001b[1;36m-0.05076908\u001b[0m,\n",
       "                \u001b[1;36m-0.017378442\u001b[0m,\n",
       "                \u001b[1;36m-0.03017162\u001b[0m,\n",
       "                \u001b[1;36m-0.01780222\u001b[0m,\n",
       "                \u001b[1;36m-0.07665155\u001b[0m,\n",
       "                \u001b[1;36m-0.046623345\u001b[0m,\n",
       "                \u001b[1;36m0.011975792\u001b[0m,\n",
       "                \u001b[1;36m-0.022626506\u001b[0m,\n",
       "                \u001b[1;36m0.037155885\u001b[0m,\n",
       "                \u001b[1;36m0.03966967\u001b[0m,\n",
       "                \u001b[1;36m0.045327567\u001b[0m,\n",
       "                \u001b[1;36m0.010550249\u001b[0m,\n",
       "                \u001b[1;36m0.046029713\u001b[0m,\n",
       "                \u001b[1;36m-0.002898502\u001b[0m,\n",
       "                \u001b[1;36m0.020544779\u001b[0m,\n",
       "                \u001b[1;36m0.028050574\u001b[0m,\n",
       "                \u001b[1;36m-0.009646118\u001b[0m,\n",
       "                \u001b[1;36m-0.014328015\u001b[0m,\n",
       "                \u001b[1;36m-0.010246864\u001b[0m,\n",
       "                \u001b[1;36m0.002703339\u001b[0m,\n",
       "                \u001b[1;36m0.04329241\u001b[0m,\n",
       "                \u001b[1;36m0.0681562\u001b[0m,\n",
       "                \u001b[1;36m0.009135491\u001b[0m,\n",
       "                \u001b[1;36m0.0671044\u001b[0m,\n",
       "                \u001b[1;36m-0.051254403\u001b[0m,\n",
       "                \u001b[1;36m-0.0380243\u001b[0m,\n",
       "                \u001b[1;36m0.031048004\u001b[0m,\n",
       "                \u001b[1;36m-0.08539608\u001b[0m,\n",
       "                \u001b[1;36m0.023425952\u001b[0m,\n",
       "                \u001b[1;36m-0.059093464\u001b[0m,\n",
       "                \u001b[1;36m0.03771963\u001b[0m,\n",
       "                \u001b[1;36m-0.005505408\u001b[0m,\n",
       "                \u001b[1;36m0.08227611\u001b[0m,\n",
       "                \u001b[1;36m0.0027852387\u001b[0m,\n",
       "                \u001b[1;36m0.02822454\u001b[0m,\n",
       "                \u001b[1;36m-0.03574911\u001b[0m,\n",
       "                \u001b[1;36m-0.088758364\u001b[0m,\n",
       "                \u001b[1;36m-0.08574517\u001b[0m,\n",
       "                \u001b[1;36m0.06668714\u001b[0m,\n",
       "                \u001b[1;36m-0.04406618\u001b[0m,\n",
       "                \u001b[1;36m-0.06357254\u001b[0m,\n",
       "                \u001b[1;36m0.09611272\u001b[0m,\n",
       "                \u001b[1;36m0.055917\u001b[0m,\n",
       "                \u001b[1;36m-0.08705364\u001b[0m,\n",
       "                \u001b[1;36m0.095189266\u001b[0m,\n",
       "                \u001b[1;36m-0.03884191\u001b[0m,\n",
       "                \u001b[1;36m-0.099543266\u001b[0m,\n",
       "                \u001b[1;36m-6.4062036e-34\u001b[0m,\n",
       "                \u001b[1;36m0.019374205\u001b[0m,\n",
       "                \u001b[1;36m0.008741152\u001b[0m,\n",
       "                \u001b[1;36m-0.024491386\u001b[0m,\n",
       "                \u001b[1;36m0.04814948\u001b[0m,\n",
       "                \u001b[1;36m-0.025727019\u001b[0m,\n",
       "                \u001b[1;36m-0.041861188\u001b[0m,\n",
       "                \u001b[1;36m0.0324116\u001b[0m,\n",
       "                \u001b[1;36m0.022669611\u001b[0m,\n",
       "                \u001b[1;36m-0.072869636\u001b[0m,\n",
       "                \u001b[1;36m0.046576407\u001b[0m,\n",
       "                \u001b[1;36m0.00093093707\u001b[0m,\n",
       "                \u001b[1;36m-0.049277034\u001b[0m,\n",
       "                \u001b[1;36m-0.0024528275\u001b[0m,\n",
       "                \u001b[1;36m-0.03657786\u001b[0m,\n",
       "                \u001b[1;36m0.07719197\u001b[0m,\n",
       "                \u001b[1;36m-0.045579806\u001b[0m,\n",
       "                \u001b[1;36m-0.029033057\u001b[0m,\n",
       "                \u001b[1;36m-0.035945218\u001b[0m,\n",
       "                \u001b[1;36m-0.08609477\u001b[0m,\n",
       "                \u001b[1;36m-0.05367174\u001b[0m,\n",
       "                \u001b[1;36m-0.03351355\u001b[0m,\n",
       "                \u001b[1;36m0.038600307\u001b[0m,\n",
       "                \u001b[1;36m0.10361258\u001b[0m,\n",
       "                \u001b[1;36m0.027757617\u001b[0m,\n",
       "                \u001b[1;36m0.06351757\u001b[0m,\n",
       "                \u001b[1;36m-0.035390947\u001b[0m,\n",
       "                \u001b[1;36m0.0014317943\u001b[0m,\n",
       "                \u001b[1;36m0.006192092\u001b[0m,\n",
       "                \u001b[1;36m-0.039058138\u001b[0m,\n",
       "                \u001b[1;36m-0.11213894\u001b[0m,\n",
       "                \u001b[1;36m-0.046052806\u001b[0m,\n",
       "                \u001b[1;36m-0.04218518\u001b[0m,\n",
       "                \u001b[1;36m-0.06636131\u001b[0m,\n",
       "                \u001b[1;36m-0.032723434\u001b[0m,\n",
       "                \u001b[1;36m-0.026828311\u001b[0m,\n",
       "                \u001b[1;36m0.027669417\u001b[0m,\n",
       "                \u001b[1;36m0.054873694\u001b[0m,\n",
       "                \u001b[1;36m-0.06730395\u001b[0m,\n",
       "                \u001b[1;36m-0.035801467\u001b[0m,\n",
       "                \u001b[1;36m-0.010242396\u001b[0m,\n",
       "                \u001b[1;36m0.002786516\u001b[0m,\n",
       "                \u001b[1;36m0.004298824\u001b[0m,\n",
       "                \u001b[1;36m-0.068057775\u001b[0m,\n",
       "                \u001b[1;36m0.036689095\u001b[0m,\n",
       "                \u001b[1;36m-0.024299202\u001b[0m,\n",
       "                \u001b[1;36m0.010829978\u001b[0m,\n",
       "                \u001b[1;36m-0.034187958\u001b[0m,\n",
       "                \u001b[1;36m-0.014378679\u001b[0m,\n",
       "                \u001b[1;36m0.010835362\u001b[0m,\n",
       "                \u001b[1;36m0.08472067\u001b[0m,\n",
       "                \u001b[1;36m-0.021373762\u001b[0m,\n",
       "                \u001b[1;36m-0.017250067\u001b[0m,\n",
       "                \u001b[1;36m0.042057704\u001b[0m,\n",
       "                \u001b[1;36m0.026164098\u001b[0m,\n",
       "                \u001b[1;36m-0.032092307\u001b[0m,\n",
       "                \u001b[1;36m-0.025745966\u001b[0m,\n",
       "                \u001b[1;36m-0.05531999\u001b[0m,\n",
       "                \u001b[1;36m0.045077786\u001b[0m,\n",
       "                \u001b[1;36m0.01748674\u001b[0m,\n",
       "                \u001b[1;36m-0.06220995\u001b[0m,\n",
       "                \u001b[1;36m0.035986185\u001b[0m,\n",
       "                \u001b[1;36m-0.0434911\u001b[0m,\n",
       "                \u001b[1;36m-0.05527629\u001b[0m,\n",
       "                \u001b[1;36m-0.08849172\u001b[0m,\n",
       "                \u001b[1;36m0.039227657\u001b[0m,\n",
       "                \u001b[1;36m0.020592552\u001b[0m,\n",
       "                \u001b[1;36m-0.004950738\u001b[0m,\n",
       "                \u001b[1;36m0.03919372\u001b[0m,\n",
       "                \u001b[1;36m-0.0054969187\u001b[0m,\n",
       "                \u001b[1;36m-0.015540091\u001b[0m,\n",
       "                \u001b[1;36m0.019606767\u001b[0m,\n",
       "                \u001b[1;36m-0.023717856\u001b[0m,\n",
       "                \u001b[1;36m-0.038650155\u001b[0m,\n",
       "                \u001b[1;36m0.010577874\u001b[0m,\n",
       "                \u001b[1;36m-0.07570042\u001b[0m,\n",
       "                \u001b[1;36m-0.027161708\u001b[0m,\n",
       "                \u001b[1;36m-0.018637853\u001b[0m,\n",
       "                \u001b[1;36m-0.009169965\u001b[0m,\n",
       "                \u001b[1;36m0.05081357\u001b[0m,\n",
       "                \u001b[1;36m-0.005626072\u001b[0m,\n",
       "                \u001b[1;36m-0.0880178\u001b[0m,\n",
       "                \u001b[1;36m-0.009923634\u001b[0m,\n",
       "                \u001b[1;36m-0.003280131\u001b[0m,\n",
       "                \u001b[1;36m0.04810389\u001b[0m,\n",
       "                \u001b[1;36m0.030830374\u001b[0m,\n",
       "                \u001b[1;36m0.06312132\u001b[0m,\n",
       "                \u001b[1;36m0.102168076\u001b[0m,\n",
       "                \u001b[1;36m0.062001597\u001b[0m,\n",
       "                \u001b[1;36m0.03150046\u001b[0m,\n",
       "                \u001b[1;36m0.032853913\u001b[0m,\n",
       "                \u001b[1;36m0.009290752\u001b[0m,\n",
       "                \u001b[1;36m-0.011255245\u001b[0m,\n",
       "                \u001b[1;36m0.005275253\u001b[0m,\n",
       "                \u001b[1;36m0.0006773811\u001b[0m,\n",
       "                \u001b[1;36m-0.024117803\u001b[0m,\n",
       "                \u001b[1;36m-4.285374e-08\u001b[0m,\n",
       "                \u001b[1;36m-0.092341095\u001b[0m,\n",
       "                \u001b[1;36m-0.07954865\u001b[0m,\n",
       "                \u001b[1;36m-0.043613728\u001b[0m,\n",
       "                \u001b[1;36m0.07328332\u001b[0m,\n",
       "                \u001b[1;36m-0.0034253467\u001b[0m,\n",
       "                \u001b[1;36m-0.07900413\u001b[0m,\n",
       "                \u001b[1;36m0.061924793\u001b[0m,\n",
       "                \u001b[1;36m0.100435086\u001b[0m,\n",
       "                \u001b[1;36m-0.0069704335\u001b[0m,\n",
       "                \u001b[1;36m0.040756296\u001b[0m,\n",
       "                \u001b[1;36m0.06753555\u001b[0m,\n",
       "                \u001b[1;36m0.018935567\u001b[0m,\n",
       "                \u001b[1;36m-0.026013983\u001b[0m,\n",
       "                \u001b[1;36m0.0011952032\u001b[0m,\n",
       "                \u001b[1;36m0.13679461\u001b[0m,\n",
       "                \u001b[1;36m-0.026989138\u001b[0m,\n",
       "                \u001b[1;36m0.050523963\u001b[0m,\n",
       "                \u001b[1;36m0.061521284\u001b[0m,\n",
       "                \u001b[1;36m0.005472806\u001b[0m,\n",
       "                \u001b[1;36m-0.041764557\u001b[0m,\n",
       "                \u001b[1;36m0.069850735\u001b[0m,\n",
       "                \u001b[1;36m0.06550046\u001b[0m,\n",
       "                \u001b[1;36m-0.0014655694\u001b[0m,\n",
       "                \u001b[1;36m0.004496941\u001b[0m,\n",
       "                \u001b[1;36m-0.00613516\u001b[0m,\n",
       "                \u001b[1;36m-0.021994136\u001b[0m,\n",
       "                \u001b[1;36m-0.020913554\u001b[0m,\n",
       "                \u001b[1;36m0.061921928\u001b[0m,\n",
       "                \u001b[1;36m0.049571835\u001b[0m,\n",
       "                \u001b[1;36m-0.023379657\u001b[0m,\n",
       "                \u001b[1;36m0.07368106\u001b[0m,\n",
       "                \u001b[1;36m0.0025017066\u001b[0m,\n",
       "                \u001b[1;36m0.038181227\u001b[0m,\n",
       "                \u001b[1;36m-0.07607061\u001b[0m,\n",
       "                \u001b[1;36m-0.03182009\u001b[0m,\n",
       "                \u001b[1;36m0.04869202\u001b[0m,\n",
       "                \u001b[1;36m-0.028599966\u001b[0m,\n",
       "                \u001b[1;36m0.034752943\u001b[0m,\n",
       "                \u001b[1;36m0.027080975\u001b[0m,\n",
       "                \u001b[1;36m0.032496396\u001b[0m,\n",
       "                \u001b[1;36m-0.0036284167\u001b[0m,\n",
       "                \u001b[1;36m-0.04125114\u001b[0m,\n",
       "                \u001b[1;36m0.042432863\u001b[0m,\n",
       "                \u001b[1;36m0.046958406\u001b[0m,\n",
       "                \u001b[1;36m-0.008400537\u001b[0m,\n",
       "                \u001b[1;36m0.026365466\u001b[0m,\n",
       "                \u001b[1;36m-0.07602573\u001b[0m,\n",
       "                \u001b[1;36m-0.07147453\u001b[0m,\n",
       "                \u001b[1;36m0.03766765\u001b[0m,\n",
       "                \u001b[1;36m-0.08318256\u001b[0m,\n",
       "                \u001b[1;36m0.036861774\u001b[0m,\n",
       "                \u001b[1;36m-0.0061818687\u001b[0m,\n",
       "                \u001b[1;36m-0.021962399\u001b[0m,\n",
       "                \u001b[1;36m0.05041582\u001b[0m,\n",
       "                \u001b[1;36m0.05621483\u001b[0m,\n",
       "                \u001b[1;36m-0.037829414\u001b[0m,\n",
       "                \u001b[1;36m0.09600329\u001b[0m,\n",
       "                \u001b[1;36m-0.017125007\u001b[0m,\n",
       "                \u001b[1;36m0.03444506\u001b[0m,\n",
       "                \u001b[1;36m-0.032913096\u001b[0m,\n",
       "                \u001b[1;36m0.099935316\u001b[0m,\n",
       "                \u001b[1;36m0.017872145\u001b[0m,\n",
       "                \u001b[1;36m-0.096478194\u001b[0m,\n",
       "                \u001b[1;36m-0.0055429\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;36m0.00406104\u001b[0m,\n",
       "                \u001b[1;36m-0.048808675\u001b[0m,\n",
       "                \u001b[1;36m-0.054821957\u001b[0m,\n",
       "                \u001b[1;36m-0.05163342\u001b[0m,\n",
       "                \u001b[1;36m0.009840118\u001b[0m,\n",
       "                \u001b[1;36m0.09178936\u001b[0m,\n",
       "                \u001b[1;36m-0.0101559\u001b[0m,\n",
       "                \u001b[1;36m0.04716209\u001b[0m,\n",
       "                \u001b[1;36m0.02568053\u001b[0m,\n",
       "                \u001b[1;36m-0.014743081\u001b[0m,\n",
       "                \u001b[1;36m-0.023843573\u001b[0m,\n",
       "                \u001b[1;36m0.03369005\u001b[0m,\n",
       "                \u001b[1;36m-0.018293295\u001b[0m,\n",
       "                \u001b[1;36m0.022487223\u001b[0m,\n",
       "                \u001b[1;36m-0.010905134\u001b[0m,\n",
       "                \u001b[1;36m0.07937251\u001b[0m,\n",
       "                \u001b[1;36m0.108631425\u001b[0m,\n",
       "                \u001b[1;36m0.018547243\u001b[0m,\n",
       "                \u001b[1;36m-0.056368362\u001b[0m,\n",
       "                \u001b[1;36m0.012521768\u001b[0m,\n",
       "                \u001b[1;36m0.06545227\u001b[0m,\n",
       "                \u001b[1;36m-0.07763688\u001b[0m,\n",
       "                \u001b[1;36m0.10424298\u001b[0m,\n",
       "                \u001b[1;36m0.017161276\u001b[0m,\n",
       "                \u001b[1;36m0.06960253\u001b[0m,\n",
       "                \u001b[1;36m-0.00437907\u001b[0m,\n",
       "                \u001b[1;36m-0.008911352\u001b[0m,\n",
       "                \u001b[1;36m-0.0038266839\u001b[0m,\n",
       "                \u001b[1;36m0.039670363\u001b[0m,\n",
       "                \u001b[1;36m-0.12211394\u001b[0m,\n",
       "                \u001b[1;36m0.06891109\u001b[0m,\n",
       "                \u001b[1;36m0.06289771\u001b[0m,\n",
       "                \u001b[1;36m0.07961002\u001b[0m,\n",
       "                \u001b[1;36m0.04354608\u001b[0m,\n",
       "                \u001b[1;36m-0.07493716\u001b[0m,\n",
       "                \u001b[1;36m0.008346766\u001b[0m,\n",
       "                \u001b[1;36m-0.05844109\u001b[0m,\n",
       "                \u001b[1;36m-0.034176067\u001b[0m,\n",
       "                \u001b[1;36m-0.06328286\u001b[0m,\n",
       "                \u001b[1;36m0.031867906\u001b[0m,\n",
       "                \u001b[1;36m0.066620894\u001b[0m,\n",
       "                \u001b[1;36m0.060622793\u001b[0m,\n",
       "                \u001b[1;36m-0.068750225\u001b[0m,\n",
       "                \u001b[1;36m0.014350068\u001b[0m,\n",
       "                \u001b[1;36m0.09729832\u001b[0m,\n",
       "                \u001b[1;36m-0.034628067\u001b[0m,\n",
       "                \u001b[1;36m0.023585148\u001b[0m,\n",
       "                \u001b[1;36m-0.06224204\u001b[0m,\n",
       "                \u001b[1;36m-0.023213899\u001b[0m,\n",
       "                \u001b[1;36m-0.030940313\u001b[0m,\n",
       "                \u001b[1;36m-0.05188855\u001b[0m,\n",
       "                \u001b[1;36m-0.009149888\u001b[0m,\n",
       "                \u001b[1;36m-0.03583717\u001b[0m,\n",
       "                \u001b[1;36m0.045438588\u001b[0m,\n",
       "                \u001b[1;36m-0.09836591\u001b[0m,\n",
       "                \u001b[1;36m-0.053864855\u001b[0m,\n",
       "                \u001b[1;36m-0.03148368\u001b[0m,\n",
       "                \u001b[1;36m-0.006111429\u001b[0m,\n",
       "                \u001b[1;36m0.021325532\u001b[0m,\n",
       "                \u001b[1;36m0.06396347\u001b[0m,\n",
       "                \u001b[1;36m-0.037958525\u001b[0m,\n",
       "                \u001b[1;36m0.015609006\u001b[0m,\n",
       "                \u001b[1;36m0.06672296\u001b[0m,\n",
       "                \u001b[1;36m0.012821037\u001b[0m,\n",
       "                \u001b[1;36m0.07735162\u001b[0m,\n",
       "                \u001b[1;36m-0.06730516\u001b[0m,\n",
       "                \u001b[1;36m0.06136182\u001b[0m,\n",
       "                \u001b[1;36m-0.025389215\u001b[0m,\n",
       "                \u001b[1;36m-0.1137226\u001b[0m,\n",
       "                \u001b[1;36m0.050520107\u001b[0m,\n",
       "                \u001b[1;36m-0.026594693\u001b[0m,\n",
       "                \u001b[1;36m-0.023067558\u001b[0m,\n",
       "                \u001b[1;36m-0.046904422\u001b[0m,\n",
       "                \u001b[1;36m-0.050398633\u001b[0m,\n",
       "                \u001b[1;36m0.0881821\u001b[0m,\n",
       "                \u001b[1;36m0.0050534597\u001b[0m,\n",
       "                \u001b[1;36m0.010243519\u001b[0m,\n",
       "                \u001b[1;36m-0.053621396\u001b[0m,\n",
       "                \u001b[1;36m0.080357976\u001b[0m,\n",
       "                \u001b[1;36m-0.04746425\u001b[0m,\n",
       "                \u001b[1;36m0.03897748\u001b[0m,\n",
       "                \u001b[1;36m-0.022939207\u001b[0m,\n",
       "                \u001b[1;36m-0.03886172\u001b[0m,\n",
       "                \u001b[1;36m0.01014672\u001b[0m,\n",
       "                \u001b[1;36m0.03948978\u001b[0m,\n",
       "                \u001b[1;36m0.005702496\u001b[0m,\n",
       "                \u001b[1;36m0.0053803157\u001b[0m,\n",
       "                \u001b[1;36m-0.03578825\u001b[0m,\n",
       "                \u001b[1;36m-0.010108946\u001b[0m,\n",
       "                \u001b[1;36m-0.023717009\u001b[0m,\n",
       "                \u001b[1;36m0.05629815\u001b[0m,\n",
       "                \u001b[1;36m-0.07863444\u001b[0m,\n",
       "                \u001b[1;36m-0.04478068\u001b[0m,\n",
       "                \u001b[1;36m0.07231891\u001b[0m,\n",
       "                \u001b[1;36m0.088722266\u001b[0m,\n",
       "                \u001b[1;36m-0.01428813\u001b[0m,\n",
       "                \u001b[1;36m-0.02099274\u001b[0m,\n",
       "                \u001b[1;36m-0.105825774\u001b[0m,\n",
       "                \u001b[1;36m-0.0030984902\u001b[0m,\n",
       "                \u001b[1;36m0.00082296424\u001b[0m,\n",
       "                \u001b[1;36m0.111134134\u001b[0m,\n",
       "                \u001b[1;36m-0.065025434\u001b[0m,\n",
       "                \u001b[1;36m-0.030771716\u001b[0m,\n",
       "                \u001b[1;36m0.08274354\u001b[0m,\n",
       "                \u001b[1;36m-0.029646\u001b[0m,\n",
       "                \u001b[1;36m-0.0027542152\u001b[0m,\n",
       "                \u001b[1;36m0.11878068\u001b[0m,\n",
       "                \u001b[1;36m0.054552376\u001b[0m,\n",
       "                \u001b[1;36m-0.09019618\u001b[0m,\n",
       "                \u001b[1;36m-0.08030197\u001b[0m,\n",
       "                \u001b[1;36m0.045539204\u001b[0m,\n",
       "                \u001b[1;36m0.032676317\u001b[0m,\n",
       "                \u001b[1;36m0.010045935\u001b[0m,\n",
       "                \u001b[1;36m0.00045448847\u001b[0m,\n",
       "                \u001b[1;36m0.057014752\u001b[0m,\n",
       "                \u001b[1;36m-0.010716465\u001b[0m,\n",
       "                \u001b[1;36m0.06508274\u001b[0m,\n",
       "                \u001b[1;36m0.08829183\u001b[0m,\n",
       "                \u001b[1;36m0.046921328\u001b[0m,\n",
       "                \u001b[1;36m-0.0941425\u001b[0m,\n",
       "                \u001b[1;36m-0.033974033\u001b[0m,\n",
       "                \u001b[1;36m-0.071631305\u001b[0m,\n",
       "                \u001b[1;36m0.043230794\u001b[0m,\n",
       "                \u001b[1;36m0.018983835\u001b[0m,\n",
       "                \u001b[1;36m-0.056854513\u001b[0m,\n",
       "                \u001b[1;36m-0.023464497\u001b[0m,\n",
       "                \u001b[1;36m-0.12349351\u001b[0m,\n",
       "                \u001b[1;36m1.41047145e-33\u001b[0m,\n",
       "                \u001b[1;36m0.09405394\u001b[0m,\n",
       "                \u001b[1;36m0.053460535\u001b[0m,\n",
       "                \u001b[1;36m-0.057672363\u001b[0m,\n",
       "                \u001b[1;36m-0.021022841\u001b[0m,\n",
       "                \u001b[1;36m-0.0437118\u001b[0m,\n",
       "                \u001b[1;36m0.0036255645\u001b[0m,\n",
       "                \u001b[1;36m0.05365767\u001b[0m,\n",
       "                \u001b[1;36m0.0017974502\u001b[0m,\n",
       "                \u001b[1;36m-0.11173805\u001b[0m,\n",
       "                \u001b[1;36m-0.021721944\u001b[0m,\n",
       "                \u001b[1;36m-0.020497402\u001b[0m,\n",
       "                \u001b[1;36m-0.03737283\u001b[0m,\n",
       "                \u001b[1;36m0.02771337\u001b[0m,\n",
       "                \u001b[1;36m0.055384185\u001b[0m,\n",
       "                \u001b[1;36m0.030681066\u001b[0m,\n",
       "                \u001b[1;36m0.014651615\u001b[0m,\n",
       "                \u001b[1;36m0.03798954\u001b[0m,\n",
       "                \u001b[1;36m0.014513726\u001b[0m,\n",
       "                \u001b[1;36m-0.08858634\u001b[0m,\n",
       "                \u001b[1;36m-0.043590903\u001b[0m,\n",
       "                \u001b[1;36m0.054311052\u001b[0m,\n",
       "                \u001b[1;36m0.01135104\u001b[0m,\n",
       "                \u001b[1;36m-0.002714408\u001b[0m,\n",
       "                \u001b[1;36m-0.015660262\u001b[0m,\n",
       "                \u001b[1;36m0.06580455\u001b[0m,\n",
       "                \u001b[1;36m0.016221873\u001b[0m,\n",
       "                \u001b[1;36m0.008818226\u001b[0m,\n",
       "                \u001b[1;36m-0.05178188\u001b[0m,\n",
       "                \u001b[1;36m-0.10335743\u001b[0m,\n",
       "                \u001b[1;36m0.0061823605\u001b[0m,\n",
       "                \u001b[1;36m-0.05804722\u001b[0m,\n",
       "                \u001b[1;36m0.0048823925\u001b[0m,\n",
       "                \u001b[1;36m0.011941615\u001b[0m,\n",
       "                \u001b[1;36m0.007956622\u001b[0m,\n",
       "                \u001b[1;36m0.016165752\u001b[0m,\n",
       "                \u001b[1;36m-0.008485277\u001b[0m,\n",
       "                \u001b[1;36m-0.017111996\u001b[0m,\n",
       "                \u001b[1;36m0.058726076\u001b[0m,\n",
       "                \u001b[1;36m0.009807942\u001b[0m,\n",
       "                \u001b[1;36m-0.09311245\u001b[0m,\n",
       "                \u001b[1;36m0.021275343\u001b[0m,\n",
       "                \u001b[1;36m0.09670583\u001b[0m,\n",
       "                \u001b[1;36m0.032374088\u001b[0m,\n",
       "                \u001b[1;36m-0.048536807\u001b[0m,\n",
       "                \u001b[1;36m-0.015776193\u001b[0m,\n",
       "                \u001b[1;36m0.008340045\u001b[0m,\n",
       "                \u001b[1;36m0.058264416\u001b[0m,\n",
       "                \u001b[1;36m0.10344821\u001b[0m,\n",
       "                \u001b[1;36m-0.05076908\u001b[0m,\n",
       "                \u001b[1;36m-0.017378442\u001b[0m,\n",
       "                \u001b[1;36m-0.03017162\u001b[0m,\n",
       "                \u001b[1;36m-0.01780222\u001b[0m,\n",
       "                \u001b[1;36m-0.07665155\u001b[0m,\n",
       "                \u001b[1;36m-0.046623345\u001b[0m,\n",
       "                \u001b[1;36m0.011975792\u001b[0m,\n",
       "                \u001b[1;36m-0.022626506\u001b[0m,\n",
       "                \u001b[1;36m0.037155885\u001b[0m,\n",
       "                \u001b[1;36m0.03966967\u001b[0m,\n",
       "                \u001b[1;36m0.045327567\u001b[0m,\n",
       "                \u001b[1;36m0.010550249\u001b[0m,\n",
       "                \u001b[1;36m0.046029713\u001b[0m,\n",
       "                \u001b[1;36m-0.002898502\u001b[0m,\n",
       "                \u001b[1;36m0.020544779\u001b[0m,\n",
       "                \u001b[1;36m0.028050574\u001b[0m,\n",
       "                \u001b[1;36m-0.009646118\u001b[0m,\n",
       "                \u001b[1;36m-0.014328015\u001b[0m,\n",
       "                \u001b[1;36m-0.010246864\u001b[0m,\n",
       "                \u001b[1;36m0.002703339\u001b[0m,\n",
       "                \u001b[1;36m0.04329241\u001b[0m,\n",
       "                \u001b[1;36m0.0681562\u001b[0m,\n",
       "                \u001b[1;36m0.009135491\u001b[0m,\n",
       "                \u001b[1;36m0.0671044\u001b[0m,\n",
       "                \u001b[1;36m-0.051254403\u001b[0m,\n",
       "                \u001b[1;36m-0.0380243\u001b[0m,\n",
       "                \u001b[1;36m0.031048004\u001b[0m,\n",
       "                \u001b[1;36m-0.08539608\u001b[0m,\n",
       "                \u001b[1;36m0.023425952\u001b[0m,\n",
       "                \u001b[1;36m-0.059093464\u001b[0m,\n",
       "                \u001b[1;36m0.03771963\u001b[0m,\n",
       "                \u001b[1;36m-0.005505408\u001b[0m,\n",
       "                \u001b[1;36m0.08227611\u001b[0m,\n",
       "                \u001b[1;36m0.0027852387\u001b[0m,\n",
       "                \u001b[1;36m0.02822454\u001b[0m,\n",
       "                \u001b[1;36m-0.03574911\u001b[0m,\n",
       "                \u001b[1;36m-0.088758364\u001b[0m,\n",
       "                \u001b[1;36m-0.08574517\u001b[0m,\n",
       "                \u001b[1;36m0.06668714\u001b[0m,\n",
       "                \u001b[1;36m-0.04406618\u001b[0m,\n",
       "                \u001b[1;36m-0.06357254\u001b[0m,\n",
       "                \u001b[1;36m0.09611272\u001b[0m,\n",
       "                \u001b[1;36m0.055917\u001b[0m,\n",
       "                \u001b[1;36m-0.08705364\u001b[0m,\n",
       "                \u001b[1;36m0.095189266\u001b[0m,\n",
       "                \u001b[1;36m-0.03884191\u001b[0m,\n",
       "                \u001b[1;36m-0.099543266\u001b[0m,\n",
       "                \u001b[1;36m-6.4062036e-34\u001b[0m,\n",
       "                \u001b[1;36m0.019374205\u001b[0m,\n",
       "                \u001b[1;36m0.008741152\u001b[0m,\n",
       "                \u001b[1;36m-0.024491386\u001b[0m,\n",
       "                \u001b[1;36m0.04814948\u001b[0m,\n",
       "                \u001b[1;36m-0.025727019\u001b[0m,\n",
       "                \u001b[1;36m-0.041861188\u001b[0m,\n",
       "                \u001b[1;36m0.0324116\u001b[0m,\n",
       "                \u001b[1;36m0.022669611\u001b[0m,\n",
       "                \u001b[1;36m-0.072869636\u001b[0m,\n",
       "                \u001b[1;36m0.046576407\u001b[0m,\n",
       "                \u001b[1;36m0.00093093707\u001b[0m,\n",
       "                \u001b[1;36m-0.049277034\u001b[0m,\n",
       "                \u001b[1;36m-0.0024528275\u001b[0m,\n",
       "                \u001b[1;36m-0.03657786\u001b[0m,\n",
       "                \u001b[1;36m0.07719197\u001b[0m,\n",
       "                \u001b[1;36m-0.045579806\u001b[0m,\n",
       "                \u001b[1;36m-0.029033057\u001b[0m,\n",
       "                \u001b[1;36m-0.035945218\u001b[0m,\n",
       "                \u001b[1;36m-0.08609477\u001b[0m,\n",
       "                \u001b[1;36m-0.05367174\u001b[0m,\n",
       "                \u001b[1;36m-0.03351355\u001b[0m,\n",
       "                \u001b[1;36m0.038600307\u001b[0m,\n",
       "                \u001b[1;36m0.10361258\u001b[0m,\n",
       "                \u001b[1;36m0.027757617\u001b[0m,\n",
       "                \u001b[1;36m0.06351757\u001b[0m,\n",
       "                \u001b[1;36m-0.035390947\u001b[0m,\n",
       "                \u001b[1;36m0.0014317943\u001b[0m,\n",
       "                \u001b[1;36m0.006192092\u001b[0m,\n",
       "                \u001b[1;36m-0.039058138\u001b[0m,\n",
       "                \u001b[1;36m-0.11213894\u001b[0m,\n",
       "                \u001b[1;36m-0.046052806\u001b[0m,\n",
       "                \u001b[1;36m-0.04218518\u001b[0m,\n",
       "                \u001b[1;36m-0.06636131\u001b[0m,\n",
       "                \u001b[1;36m-0.032723434\u001b[0m,\n",
       "                \u001b[1;36m-0.026828311\u001b[0m,\n",
       "                \u001b[1;36m0.027669417\u001b[0m,\n",
       "                \u001b[1;36m0.054873694\u001b[0m,\n",
       "                \u001b[1;36m-0.06730395\u001b[0m,\n",
       "                \u001b[1;36m-0.035801467\u001b[0m,\n",
       "                \u001b[1;36m-0.010242396\u001b[0m,\n",
       "                \u001b[1;36m0.002786516\u001b[0m,\n",
       "                \u001b[1;36m0.004298824\u001b[0m,\n",
       "                \u001b[1;36m-0.068057775\u001b[0m,\n",
       "                \u001b[1;36m0.036689095\u001b[0m,\n",
       "                \u001b[1;36m-0.024299202\u001b[0m,\n",
       "                \u001b[1;36m0.010829978\u001b[0m,\n",
       "                \u001b[1;36m-0.034187958\u001b[0m,\n",
       "                \u001b[1;36m-0.014378679\u001b[0m,\n",
       "                \u001b[1;36m0.010835362\u001b[0m,\n",
       "                \u001b[1;36m0.08472067\u001b[0m,\n",
       "                \u001b[1;36m-0.021373762\u001b[0m,\n",
       "                \u001b[1;36m-0.017250067\u001b[0m,\n",
       "                \u001b[1;36m0.042057704\u001b[0m,\n",
       "                \u001b[1;36m0.026164098\u001b[0m,\n",
       "                \u001b[1;36m-0.032092307\u001b[0m,\n",
       "                \u001b[1;36m-0.025745966\u001b[0m,\n",
       "                \u001b[1;36m-0.05531999\u001b[0m,\n",
       "                \u001b[1;36m0.045077786\u001b[0m,\n",
       "                \u001b[1;36m0.01748674\u001b[0m,\n",
       "                \u001b[1;36m-0.06220995\u001b[0m,\n",
       "                \u001b[1;36m0.035986185\u001b[0m,\n",
       "                \u001b[1;36m-0.0434911\u001b[0m,\n",
       "                \u001b[1;36m-0.05527629\u001b[0m,\n",
       "                \u001b[1;36m-0.08849172\u001b[0m,\n",
       "                \u001b[1;36m0.039227657\u001b[0m,\n",
       "                \u001b[1;36m0.020592552\u001b[0m,\n",
       "                \u001b[1;36m-0.004950738\u001b[0m,\n",
       "                \u001b[1;36m0.03919372\u001b[0m,\n",
       "                \u001b[1;36m-0.0054969187\u001b[0m,\n",
       "                \u001b[1;36m-0.015540091\u001b[0m,\n",
       "                \u001b[1;36m0.019606767\u001b[0m,\n",
       "                \u001b[1;36m-0.023717856\u001b[0m,\n",
       "                \u001b[1;36m-0.038650155\u001b[0m,\n",
       "                \u001b[1;36m0.010577874\u001b[0m,\n",
       "                \u001b[1;36m-0.07570042\u001b[0m,\n",
       "                \u001b[1;36m-0.027161708\u001b[0m,\n",
       "                \u001b[1;36m-0.018637853\u001b[0m,\n",
       "                \u001b[1;36m-0.009169965\u001b[0m,\n",
       "                \u001b[1;36m0.05081357\u001b[0m,\n",
       "                \u001b[1;36m-0.005626072\u001b[0m,\n",
       "                \u001b[1;36m-0.0880178\u001b[0m,\n",
       "                \u001b[1;36m-0.009923634\u001b[0m,\n",
       "                \u001b[1;36m-0.003280131\u001b[0m,\n",
       "                \u001b[1;36m0.04810389\u001b[0m,\n",
       "                \u001b[1;36m0.030830374\u001b[0m,\n",
       "                \u001b[1;36m0.06312132\u001b[0m,\n",
       "                \u001b[1;36m0.102168076\u001b[0m,\n",
       "                \u001b[1;36m0.062001597\u001b[0m,\n",
       "                \u001b[1;36m0.03150046\u001b[0m,\n",
       "                \u001b[1;36m0.032853913\u001b[0m,\n",
       "                \u001b[1;36m0.009290752\u001b[0m,\n",
       "                \u001b[1;36m-0.011255245\u001b[0m,\n",
       "                \u001b[1;36m0.005275253\u001b[0m,\n",
       "                \u001b[1;36m0.0006773811\u001b[0m,\n",
       "                \u001b[1;36m-0.024117803\u001b[0m,\n",
       "                \u001b[1;36m-4.285374e-08\u001b[0m,\n",
       "                \u001b[1;36m-0.092341095\u001b[0m,\n",
       "                \u001b[1;36m-0.07954865\u001b[0m,\n",
       "                \u001b[1;36m-0.043613728\u001b[0m,\n",
       "                \u001b[1;36m0.07328332\u001b[0m,\n",
       "                \u001b[1;36m-0.0034253467\u001b[0m,\n",
       "                \u001b[1;36m-0.07900413\u001b[0m,\n",
       "                \u001b[1;36m0.061924793\u001b[0m,\n",
       "                \u001b[1;36m0.100435086\u001b[0m,\n",
       "                \u001b[1;36m-0.0069704335\u001b[0m,\n",
       "                \u001b[1;36m0.040756296\u001b[0m,\n",
       "                \u001b[1;36m0.06753555\u001b[0m,\n",
       "                \u001b[1;36m0.018935567\u001b[0m,\n",
       "                \u001b[1;36m-0.026013983\u001b[0m,\n",
       "                \u001b[1;36m0.0011952032\u001b[0m,\n",
       "                \u001b[1;36m0.13679461\u001b[0m,\n",
       "                \u001b[1;36m-0.026989138\u001b[0m,\n",
       "                \u001b[1;36m0.050523963\u001b[0m,\n",
       "                \u001b[1;36m0.061521284\u001b[0m,\n",
       "                \u001b[1;36m0.005472806\u001b[0m,\n",
       "                \u001b[1;36m-0.041764557\u001b[0m,\n",
       "                \u001b[1;36m0.069850735\u001b[0m,\n",
       "                \u001b[1;36m0.06550046\u001b[0m,\n",
       "                \u001b[1;36m-0.0014655694\u001b[0m,\n",
       "                \u001b[1;36m0.004496941\u001b[0m,\n",
       "                \u001b[1;36m-0.00613516\u001b[0m,\n",
       "                \u001b[1;36m-0.021994136\u001b[0m,\n",
       "                \u001b[1;36m-0.020913554\u001b[0m,\n",
       "                \u001b[1;36m0.061921928\u001b[0m,\n",
       "                \u001b[1;36m0.049571835\u001b[0m,\n",
       "                \u001b[1;36m-0.023379657\u001b[0m,\n",
       "                \u001b[1;36m0.07368106\u001b[0m,\n",
       "                \u001b[1;36m0.0025017066\u001b[0m,\n",
       "                \u001b[1;36m0.038181227\u001b[0m,\n",
       "                \u001b[1;36m-0.07607061\u001b[0m,\n",
       "                \u001b[1;36m-0.03182009\u001b[0m,\n",
       "                \u001b[1;36m0.04869202\u001b[0m,\n",
       "                \u001b[1;36m-0.028599966\u001b[0m,\n",
       "                \u001b[1;36m0.034752943\u001b[0m,\n",
       "                \u001b[1;36m0.027080975\u001b[0m,\n",
       "                \u001b[1;36m0.032496396\u001b[0m,\n",
       "                \u001b[1;36m-0.0036284167\u001b[0m,\n",
       "                \u001b[1;36m-0.04125114\u001b[0m,\n",
       "                \u001b[1;36m0.042432863\u001b[0m,\n",
       "                \u001b[1;36m0.046958406\u001b[0m,\n",
       "                \u001b[1;36m-0.008400537\u001b[0m,\n",
       "                \u001b[1;36m0.026365466\u001b[0m,\n",
       "                \u001b[1;36m-0.07602573\u001b[0m,\n",
       "                \u001b[1;36m-0.07147453\u001b[0m,\n",
       "                \u001b[1;36m0.03766765\u001b[0m,\n",
       "                \u001b[1;36m-0.08318256\u001b[0m,\n",
       "                \u001b[1;36m0.036861774\u001b[0m,\n",
       "                \u001b[1;36m-0.0061818687\u001b[0m,\n",
       "                \u001b[1;36m-0.021962399\u001b[0m,\n",
       "                \u001b[1;36m0.05041582\u001b[0m,\n",
       "                \u001b[1;36m0.05621483\u001b[0m,\n",
       "                \u001b[1;36m-0.037829414\u001b[0m,\n",
       "                \u001b[1;36m0.09600329\u001b[0m,\n",
       "                \u001b[1;36m-0.017125007\u001b[0m,\n",
       "                \u001b[1;36m0.03444506\u001b[0m,\n",
       "                \u001b[1;36m-0.032913096\u001b[0m,\n",
       "                \u001b[1;36m0.099935316\u001b[0m,\n",
       "                \u001b[1;36m0.017872145\u001b[0m,\n",
       "                \u001b[1;36m-0.096478194\u001b[0m,\n",
       "                \u001b[1;36m-0.0055429\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m',\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe preceding command will run a \u001b[0m\n",
       "\u001b[32mLoRA finetune with torchtune\\'s factory settings, but we may want to experiment a bit.\\nLet\\'s take a closer look \u001b[0m\n",
       "\u001b[32mat some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n\u001b[0m\n",
       "\u001b[32m_component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: \u001b[0m\n",
       "\u001b[32m16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V projections with a rank of 8.\\nSome experiments \u001b[0m\n",
       "\u001b[32mwith LoRA have found that it can be beneficial to apply LoRA to all linear layers in\\nthe self-attention, and to \u001b[0m\n",
       "\u001b[32mincrease the rank to 16 or 32. Note that this is likely to increase our max memory,\\nbut as long as we keep \u001b[0m\n",
       "\u001b[32m:code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s run this experiment. We can also increase\u001b[0m\n",
       "\u001b[32malpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run \u001b[0m\n",
       "\u001b[32m--nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our baseline over\u001b[0m\n",
       "\u001b[32mthe first 500 steps can be seen below.\\n\\n.. image:: /_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n   \u001b[0m\n",
       "\u001b[32mThe above figure was generated with W&B. You can use torchtune\\'s \u001b[0m\n",
       "\u001b[32m:class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you will need to \u001b[0m\n",
       "\u001b[32minstall W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'document_id'\u001b[0m: \u001b[32m'num-3'\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m512.0\u001b[0m, \u001b[32m'metadata_token_count'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33membedding\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;36m0.00406104\u001b[0m,\n",
       "                \u001b[1;36m-0.048808675\u001b[0m,\n",
       "                \u001b[1;36m-0.054821957\u001b[0m,\n",
       "                \u001b[1;36m-0.05163342\u001b[0m,\n",
       "                \u001b[1;36m0.009840118\u001b[0m,\n",
       "                \u001b[1;36m0.09178936\u001b[0m,\n",
       "                \u001b[1;36m-0.0101559\u001b[0m,\n",
       "                \u001b[1;36m0.04716209\u001b[0m,\n",
       "                \u001b[1;36m0.02568053\u001b[0m,\n",
       "                \u001b[1;36m-0.014743081\u001b[0m,\n",
       "                \u001b[1;36m-0.023843573\u001b[0m,\n",
       "                \u001b[1;36m0.03369005\u001b[0m,\n",
       "                \u001b[1;36m-0.018293295\u001b[0m,\n",
       "                \u001b[1;36m0.022487223\u001b[0m,\n",
       "                \u001b[1;36m-0.010905134\u001b[0m,\n",
       "                \u001b[1;36m0.07937251\u001b[0m,\n",
       "                \u001b[1;36m0.108631425\u001b[0m,\n",
       "                \u001b[1;36m0.018547243\u001b[0m,\n",
       "                \u001b[1;36m-0.056368362\u001b[0m,\n",
       "                \u001b[1;36m0.012521768\u001b[0m,\n",
       "                \u001b[1;36m0.06545227\u001b[0m,\n",
       "                \u001b[1;36m-0.07763688\u001b[0m,\n",
       "                \u001b[1;36m0.10424298\u001b[0m,\n",
       "                \u001b[1;36m0.017161276\u001b[0m,\n",
       "                \u001b[1;36m0.06960253\u001b[0m,\n",
       "                \u001b[1;36m-0.00437907\u001b[0m,\n",
       "                \u001b[1;36m-0.008911352\u001b[0m,\n",
       "                \u001b[1;36m-0.0038266839\u001b[0m,\n",
       "                \u001b[1;36m0.039670363\u001b[0m,\n",
       "                \u001b[1;36m-0.12211394\u001b[0m,\n",
       "                \u001b[1;36m0.06891109\u001b[0m,\n",
       "                \u001b[1;36m0.06289771\u001b[0m,\n",
       "                \u001b[1;36m0.07961002\u001b[0m,\n",
       "                \u001b[1;36m0.04354608\u001b[0m,\n",
       "                \u001b[1;36m-0.07493716\u001b[0m,\n",
       "                \u001b[1;36m0.008346766\u001b[0m,\n",
       "                \u001b[1;36m-0.05844109\u001b[0m,\n",
       "                \u001b[1;36m-0.034176067\u001b[0m,\n",
       "                \u001b[1;36m-0.06328286\u001b[0m,\n",
       "                \u001b[1;36m0.031867906\u001b[0m,\n",
       "                \u001b[1;36m0.066620894\u001b[0m,\n",
       "                \u001b[1;36m0.060622793\u001b[0m,\n",
       "                \u001b[1;36m-0.068750225\u001b[0m,\n",
       "                \u001b[1;36m0.014350068\u001b[0m,\n",
       "                \u001b[1;36m0.09729832\u001b[0m,\n",
       "                \u001b[1;36m-0.034628067\u001b[0m,\n",
       "                \u001b[1;36m0.023585148\u001b[0m,\n",
       "                \u001b[1;36m-0.06224204\u001b[0m,\n",
       "                \u001b[1;36m-0.023213899\u001b[0m,\n",
       "                \u001b[1;36m-0.030940313\u001b[0m,\n",
       "                \u001b[1;36m-0.05188855\u001b[0m,\n",
       "                \u001b[1;36m-0.009149888\u001b[0m,\n",
       "                \u001b[1;36m-0.03583717\u001b[0m,\n",
       "                \u001b[1;36m0.045438588\u001b[0m,\n",
       "                \u001b[1;36m-0.09836591\u001b[0m,\n",
       "                \u001b[1;36m-0.053864855\u001b[0m,\n",
       "                \u001b[1;36m-0.03148368\u001b[0m,\n",
       "                \u001b[1;36m-0.006111429\u001b[0m,\n",
       "                \u001b[1;36m0.021325532\u001b[0m,\n",
       "                \u001b[1;36m0.06396347\u001b[0m,\n",
       "                \u001b[1;36m-0.037958525\u001b[0m,\n",
       "                \u001b[1;36m0.015609006\u001b[0m,\n",
       "                \u001b[1;36m0.06672296\u001b[0m,\n",
       "                \u001b[1;36m0.012821037\u001b[0m,\n",
       "                \u001b[1;36m0.07735162\u001b[0m,\n",
       "                \u001b[1;36m-0.06730516\u001b[0m,\n",
       "                \u001b[1;36m0.06136182\u001b[0m,\n",
       "                \u001b[1;36m-0.025389215\u001b[0m,\n",
       "                \u001b[1;36m-0.1137226\u001b[0m,\n",
       "                \u001b[1;36m0.050520107\u001b[0m,\n",
       "                \u001b[1;36m-0.026594693\u001b[0m,\n",
       "                \u001b[1;36m-0.023067558\u001b[0m,\n",
       "                \u001b[1;36m-0.046904422\u001b[0m,\n",
       "                \u001b[1;36m-0.050398633\u001b[0m,\n",
       "                \u001b[1;36m0.0881821\u001b[0m,\n",
       "                \u001b[1;36m0.0050534597\u001b[0m,\n",
       "                \u001b[1;36m0.010243519\u001b[0m,\n",
       "                \u001b[1;36m-0.053621396\u001b[0m,\n",
       "                \u001b[1;36m0.080357976\u001b[0m,\n",
       "                \u001b[1;36m-0.04746425\u001b[0m,\n",
       "                \u001b[1;36m0.03897748\u001b[0m,\n",
       "                \u001b[1;36m-0.022939207\u001b[0m,\n",
       "                \u001b[1;36m-0.03886172\u001b[0m,\n",
       "                \u001b[1;36m0.01014672\u001b[0m,\n",
       "                \u001b[1;36m0.03948978\u001b[0m,\n",
       "                \u001b[1;36m0.005702496\u001b[0m,\n",
       "                \u001b[1;36m0.0053803157\u001b[0m,\n",
       "                \u001b[1;36m-0.03578825\u001b[0m,\n",
       "                \u001b[1;36m-0.010108946\u001b[0m,\n",
       "                \u001b[1;36m-0.023717009\u001b[0m,\n",
       "                \u001b[1;36m0.05629815\u001b[0m,\n",
       "                \u001b[1;36m-0.07863444\u001b[0m,\n",
       "                \u001b[1;36m-0.04478068\u001b[0m,\n",
       "                \u001b[1;36m0.07231891\u001b[0m,\n",
       "                \u001b[1;36m0.088722266\u001b[0m,\n",
       "                \u001b[1;36m-0.01428813\u001b[0m,\n",
       "                \u001b[1;36m-0.02099274\u001b[0m,\n",
       "                \u001b[1;36m-0.105825774\u001b[0m,\n",
       "                \u001b[1;36m-0.0030984902\u001b[0m,\n",
       "                \u001b[1;36m0.00082296424\u001b[0m,\n",
       "                \u001b[1;36m0.111134134\u001b[0m,\n",
       "                \u001b[1;36m-0.065025434\u001b[0m,\n",
       "                \u001b[1;36m-0.030771716\u001b[0m,\n",
       "                \u001b[1;36m0.08274354\u001b[0m,\n",
       "                \u001b[1;36m-0.029646\u001b[0m,\n",
       "                \u001b[1;36m-0.0027542152\u001b[0m,\n",
       "                \u001b[1;36m0.11878068\u001b[0m,\n",
       "                \u001b[1;36m0.054552376\u001b[0m,\n",
       "                \u001b[1;36m-0.09019618\u001b[0m,\n",
       "                \u001b[1;36m-0.08030197\u001b[0m,\n",
       "                \u001b[1;36m0.045539204\u001b[0m,\n",
       "                \u001b[1;36m0.032676317\u001b[0m,\n",
       "                \u001b[1;36m0.010045935\u001b[0m,\n",
       "                \u001b[1;36m0.00045448847\u001b[0m,\n",
       "                \u001b[1;36m0.057014752\u001b[0m,\n",
       "                \u001b[1;36m-0.010716465\u001b[0m,\n",
       "                \u001b[1;36m0.06508274\u001b[0m,\n",
       "                \u001b[1;36m0.08829183\u001b[0m,\n",
       "                \u001b[1;36m0.046921328\u001b[0m,\n",
       "                \u001b[1;36m-0.0941425\u001b[0m,\n",
       "                \u001b[1;36m-0.033974033\u001b[0m,\n",
       "                \u001b[1;36m-0.071631305\u001b[0m,\n",
       "                \u001b[1;36m0.043230794\u001b[0m,\n",
       "                \u001b[1;36m0.018983835\u001b[0m,\n",
       "                \u001b[1;36m-0.056854513\u001b[0m,\n",
       "                \u001b[1;36m-0.023464497\u001b[0m,\n",
       "                \u001b[1;36m-0.12349351\u001b[0m,\n",
       "                \u001b[1;36m1.41047145e-33\u001b[0m,\n",
       "                \u001b[1;36m0.09405394\u001b[0m,\n",
       "                \u001b[1;36m0.053460535\u001b[0m,\n",
       "                \u001b[1;36m-0.057672363\u001b[0m,\n",
       "                \u001b[1;36m-0.021022841\u001b[0m,\n",
       "                \u001b[1;36m-0.0437118\u001b[0m,\n",
       "                \u001b[1;36m0.0036255645\u001b[0m,\n",
       "                \u001b[1;36m0.05365767\u001b[0m,\n",
       "                \u001b[1;36m0.0017974502\u001b[0m,\n",
       "                \u001b[1;36m-0.11173805\u001b[0m,\n",
       "                \u001b[1;36m-0.021721944\u001b[0m,\n",
       "                \u001b[1;36m-0.020497402\u001b[0m,\n",
       "                \u001b[1;36m-0.03737283\u001b[0m,\n",
       "                \u001b[1;36m0.02771337\u001b[0m,\n",
       "                \u001b[1;36m0.055384185\u001b[0m,\n",
       "                \u001b[1;36m0.030681066\u001b[0m,\n",
       "                \u001b[1;36m0.014651615\u001b[0m,\n",
       "                \u001b[1;36m0.03798954\u001b[0m,\n",
       "                \u001b[1;36m0.014513726\u001b[0m,\n",
       "                \u001b[1;36m-0.08858634\u001b[0m,\n",
       "                \u001b[1;36m-0.043590903\u001b[0m,\n",
       "                \u001b[1;36m0.054311052\u001b[0m,\n",
       "                \u001b[1;36m0.01135104\u001b[0m,\n",
       "                \u001b[1;36m-0.002714408\u001b[0m,\n",
       "                \u001b[1;36m-0.015660262\u001b[0m,\n",
       "                \u001b[1;36m0.06580455\u001b[0m,\n",
       "                \u001b[1;36m0.016221873\u001b[0m,\n",
       "                \u001b[1;36m0.008818226\u001b[0m,\n",
       "                \u001b[1;36m-0.05178188\u001b[0m,\n",
       "                \u001b[1;36m-0.10335743\u001b[0m,\n",
       "                \u001b[1;36m0.0061823605\u001b[0m,\n",
       "                \u001b[1;36m-0.05804722\u001b[0m,\n",
       "                \u001b[1;36m0.0048823925\u001b[0m,\n",
       "                \u001b[1;36m0.011941615\u001b[0m,\n",
       "                \u001b[1;36m0.007956622\u001b[0m,\n",
       "                \u001b[1;36m0.016165752\u001b[0m,\n",
       "                \u001b[1;36m-0.008485277\u001b[0m,\n",
       "                \u001b[1;36m-0.017111996\u001b[0m,\n",
       "                \u001b[1;36m0.058726076\u001b[0m,\n",
       "                \u001b[1;36m0.009807942\u001b[0m,\n",
       "                \u001b[1;36m-0.09311245\u001b[0m,\n",
       "                \u001b[1;36m0.021275343\u001b[0m,\n",
       "                \u001b[1;36m0.09670583\u001b[0m,\n",
       "                \u001b[1;36m0.032374088\u001b[0m,\n",
       "                \u001b[1;36m-0.048536807\u001b[0m,\n",
       "                \u001b[1;36m-0.015776193\u001b[0m,\n",
       "                \u001b[1;36m0.008340045\u001b[0m,\n",
       "                \u001b[1;36m0.058264416\u001b[0m,\n",
       "                \u001b[1;36m0.10344821\u001b[0m,\n",
       "                \u001b[1;36m-0.05076908\u001b[0m,\n",
       "                \u001b[1;36m-0.017378442\u001b[0m,\n",
       "                \u001b[1;36m-0.03017162\u001b[0m,\n",
       "                \u001b[1;36m-0.01780222\u001b[0m,\n",
       "                \u001b[1;36m-0.07665155\u001b[0m,\n",
       "                \u001b[1;36m-0.046623345\u001b[0m,\n",
       "                \u001b[1;36m0.011975792\u001b[0m,\n",
       "                \u001b[1;36m-0.022626506\u001b[0m,\n",
       "                \u001b[1;36m0.037155885\u001b[0m,\n",
       "                \u001b[1;36m0.03966967\u001b[0m,\n",
       "                \u001b[1;36m0.045327567\u001b[0m,\n",
       "                \u001b[1;36m0.010550249\u001b[0m,\n",
       "                \u001b[1;36m0.046029713\u001b[0m,\n",
       "                \u001b[1;36m-0.002898502\u001b[0m,\n",
       "                \u001b[1;36m0.020544779\u001b[0m,\n",
       "                \u001b[1;36m0.028050574\u001b[0m,\n",
       "                \u001b[1;36m-0.009646118\u001b[0m,\n",
       "                \u001b[1;36m-0.014328015\u001b[0m,\n",
       "                \u001b[1;36m-0.010246864\u001b[0m,\n",
       "                \u001b[1;36m0.002703339\u001b[0m,\n",
       "                \u001b[1;36m0.04329241\u001b[0m,\n",
       "                \u001b[1;36m0.0681562\u001b[0m,\n",
       "                \u001b[1;36m0.009135491\u001b[0m,\n",
       "                \u001b[1;36m0.0671044\u001b[0m,\n",
       "                \u001b[1;36m-0.051254403\u001b[0m,\n",
       "                \u001b[1;36m-0.0380243\u001b[0m,\n",
       "                \u001b[1;36m0.031048004\u001b[0m,\n",
       "                \u001b[1;36m-0.08539608\u001b[0m,\n",
       "                \u001b[1;36m0.023425952\u001b[0m,\n",
       "                \u001b[1;36m-0.059093464\u001b[0m,\n",
       "                \u001b[1;36m0.03771963\u001b[0m,\n",
       "                \u001b[1;36m-0.005505408\u001b[0m,\n",
       "                \u001b[1;36m0.08227611\u001b[0m,\n",
       "                \u001b[1;36m0.0027852387\u001b[0m,\n",
       "                \u001b[1;36m0.02822454\u001b[0m,\n",
       "                \u001b[1;36m-0.03574911\u001b[0m,\n",
       "                \u001b[1;36m-0.088758364\u001b[0m,\n",
       "                \u001b[1;36m-0.08574517\u001b[0m,\n",
       "                \u001b[1;36m0.06668714\u001b[0m,\n",
       "                \u001b[1;36m-0.04406618\u001b[0m,\n",
       "                \u001b[1;36m-0.06357254\u001b[0m,\n",
       "                \u001b[1;36m0.09611272\u001b[0m,\n",
       "                \u001b[1;36m0.055917\u001b[0m,\n",
       "                \u001b[1;36m-0.08705364\u001b[0m,\n",
       "                \u001b[1;36m0.095189266\u001b[0m,\n",
       "                \u001b[1;36m-0.03884191\u001b[0m,\n",
       "                \u001b[1;36m-0.099543266\u001b[0m,\n",
       "                \u001b[1;36m-6.4062036e-34\u001b[0m,\n",
       "                \u001b[1;36m0.019374205\u001b[0m,\n",
       "                \u001b[1;36m0.008741152\u001b[0m,\n",
       "                \u001b[1;36m-0.024491386\u001b[0m,\n",
       "                \u001b[1;36m0.04814948\u001b[0m,\n",
       "                \u001b[1;36m-0.025727019\u001b[0m,\n",
       "                \u001b[1;36m-0.041861188\u001b[0m,\n",
       "                \u001b[1;36m0.0324116\u001b[0m,\n",
       "                \u001b[1;36m0.022669611\u001b[0m,\n",
       "                \u001b[1;36m-0.072869636\u001b[0m,\n",
       "                \u001b[1;36m0.046576407\u001b[0m,\n",
       "                \u001b[1;36m0.00093093707\u001b[0m,\n",
       "                \u001b[1;36m-0.049277034\u001b[0m,\n",
       "                \u001b[1;36m-0.0024528275\u001b[0m,\n",
       "                \u001b[1;36m-0.03657786\u001b[0m,\n",
       "                \u001b[1;36m0.07719197\u001b[0m,\n",
       "                \u001b[1;36m-0.045579806\u001b[0m,\n",
       "                \u001b[1;36m-0.029033057\u001b[0m,\n",
       "                \u001b[1;36m-0.035945218\u001b[0m,\n",
       "                \u001b[1;36m-0.08609477\u001b[0m,\n",
       "                \u001b[1;36m-0.05367174\u001b[0m,\n",
       "                \u001b[1;36m-0.03351355\u001b[0m,\n",
       "                \u001b[1;36m0.038600307\u001b[0m,\n",
       "                \u001b[1;36m0.10361258\u001b[0m,\n",
       "                \u001b[1;36m0.027757617\u001b[0m,\n",
       "                \u001b[1;36m0.06351757\u001b[0m,\n",
       "                \u001b[1;36m-0.035390947\u001b[0m,\n",
       "                \u001b[1;36m0.0014317943\u001b[0m,\n",
       "                \u001b[1;36m0.006192092\u001b[0m,\n",
       "                \u001b[1;36m-0.039058138\u001b[0m,\n",
       "                \u001b[1;36m-0.11213894\u001b[0m,\n",
       "                \u001b[1;36m-0.046052806\u001b[0m,\n",
       "                \u001b[1;36m-0.04218518\u001b[0m,\n",
       "                \u001b[1;36m-0.06636131\u001b[0m,\n",
       "                \u001b[1;36m-0.032723434\u001b[0m,\n",
       "                \u001b[1;36m-0.026828311\u001b[0m,\n",
       "                \u001b[1;36m0.027669417\u001b[0m,\n",
       "                \u001b[1;36m0.054873694\u001b[0m,\n",
       "                \u001b[1;36m-0.06730395\u001b[0m,\n",
       "                \u001b[1;36m-0.035801467\u001b[0m,\n",
       "                \u001b[1;36m-0.010242396\u001b[0m,\n",
       "                \u001b[1;36m0.002786516\u001b[0m,\n",
       "                \u001b[1;36m0.004298824\u001b[0m,\n",
       "                \u001b[1;36m-0.068057775\u001b[0m,\n",
       "                \u001b[1;36m0.036689095\u001b[0m,\n",
       "                \u001b[1;36m-0.024299202\u001b[0m,\n",
       "                \u001b[1;36m0.010829978\u001b[0m,\n",
       "                \u001b[1;36m-0.034187958\u001b[0m,\n",
       "                \u001b[1;36m-0.014378679\u001b[0m,\n",
       "                \u001b[1;36m0.010835362\u001b[0m,\n",
       "                \u001b[1;36m0.08472067\u001b[0m,\n",
       "                \u001b[1;36m-0.021373762\u001b[0m,\n",
       "                \u001b[1;36m-0.017250067\u001b[0m,\n",
       "                \u001b[1;36m0.042057704\u001b[0m,\n",
       "                \u001b[1;36m0.026164098\u001b[0m,\n",
       "                \u001b[1;36m-0.032092307\u001b[0m,\n",
       "                \u001b[1;36m-0.025745966\u001b[0m,\n",
       "                \u001b[1;36m-0.05531999\u001b[0m,\n",
       "                \u001b[1;36m0.045077786\u001b[0m,\n",
       "                \u001b[1;36m0.01748674\u001b[0m,\n",
       "                \u001b[1;36m-0.06220995\u001b[0m,\n",
       "                \u001b[1;36m0.035986185\u001b[0m,\n",
       "                \u001b[1;36m-0.0434911\u001b[0m,\n",
       "                \u001b[1;36m-0.05527629\u001b[0m,\n",
       "                \u001b[1;36m-0.08849172\u001b[0m,\n",
       "                \u001b[1;36m0.039227657\u001b[0m,\n",
       "                \u001b[1;36m0.020592552\u001b[0m,\n",
       "                \u001b[1;36m-0.004950738\u001b[0m,\n",
       "                \u001b[1;36m0.03919372\u001b[0m,\n",
       "                \u001b[1;36m-0.0054969187\u001b[0m,\n",
       "                \u001b[1;36m-0.015540091\u001b[0m,\n",
       "                \u001b[1;36m0.019606767\u001b[0m,\n",
       "                \u001b[1;36m-0.023717856\u001b[0m,\n",
       "                \u001b[1;36m-0.038650155\u001b[0m,\n",
       "                \u001b[1;36m0.010577874\u001b[0m,\n",
       "                \u001b[1;36m-0.07570042\u001b[0m,\n",
       "                \u001b[1;36m-0.027161708\u001b[0m,\n",
       "                \u001b[1;36m-0.018637853\u001b[0m,\n",
       "                \u001b[1;36m-0.009169965\u001b[0m,\n",
       "                \u001b[1;36m0.05081357\u001b[0m,\n",
       "                \u001b[1;36m-0.005626072\u001b[0m,\n",
       "                \u001b[1;36m-0.0880178\u001b[0m,\n",
       "                \u001b[1;36m-0.009923634\u001b[0m,\n",
       "                \u001b[1;36m-0.003280131\u001b[0m,\n",
       "                \u001b[1;36m0.04810389\u001b[0m,\n",
       "                \u001b[1;36m0.030830374\u001b[0m,\n",
       "                \u001b[1;36m0.06312132\u001b[0m,\n",
       "                \u001b[1;36m0.102168076\u001b[0m,\n",
       "                \u001b[1;36m0.062001597\u001b[0m,\n",
       "                \u001b[1;36m0.03150046\u001b[0m,\n",
       "                \u001b[1;36m0.032853913\u001b[0m,\n",
       "                \u001b[1;36m0.009290752\u001b[0m,\n",
       "                \u001b[1;36m-0.011255245\u001b[0m,\n",
       "                \u001b[1;36m0.005275253\u001b[0m,\n",
       "                \u001b[1;36m0.0006773811\u001b[0m,\n",
       "                \u001b[1;36m-0.024117803\u001b[0m,\n",
       "                \u001b[1;36m-4.285374e-08\u001b[0m,\n",
       "                \u001b[1;36m-0.092341095\u001b[0m,\n",
       "                \u001b[1;36m-0.07954865\u001b[0m,\n",
       "                \u001b[1;36m-0.043613728\u001b[0m,\n",
       "                \u001b[1;36m0.07328332\u001b[0m,\n",
       "                \u001b[1;36m-0.0034253467\u001b[0m,\n",
       "                \u001b[1;36m-0.07900413\u001b[0m,\n",
       "                \u001b[1;36m0.061924793\u001b[0m,\n",
       "                \u001b[1;36m0.100435086\u001b[0m,\n",
       "                \u001b[1;36m-0.0069704335\u001b[0m,\n",
       "                \u001b[1;36m0.040756296\u001b[0m,\n",
       "                \u001b[1;36m0.06753555\u001b[0m,\n",
       "                \u001b[1;36m0.018935567\u001b[0m,\n",
       "                \u001b[1;36m-0.026013983\u001b[0m,\n",
       "                \u001b[1;36m0.0011952032\u001b[0m,\n",
       "                \u001b[1;36m0.13679461\u001b[0m,\n",
       "                \u001b[1;36m-0.026989138\u001b[0m,\n",
       "                \u001b[1;36m0.050523963\u001b[0m,\n",
       "                \u001b[1;36m0.061521284\u001b[0m,\n",
       "                \u001b[1;36m0.005472806\u001b[0m,\n",
       "                \u001b[1;36m-0.041764557\u001b[0m,\n",
       "                \u001b[1;36m0.069850735\u001b[0m,\n",
       "                \u001b[1;36m0.06550046\u001b[0m,\n",
       "                \u001b[1;36m-0.0014655694\u001b[0m,\n",
       "                \u001b[1;36m0.004496941\u001b[0m,\n",
       "                \u001b[1;36m-0.00613516\u001b[0m,\n",
       "                \u001b[1;36m-0.021994136\u001b[0m,\n",
       "                \u001b[1;36m-0.020913554\u001b[0m,\n",
       "                \u001b[1;36m0.061921928\u001b[0m,\n",
       "                \u001b[1;36m0.049571835\u001b[0m,\n",
       "                \u001b[1;36m-0.023379657\u001b[0m,\n",
       "                \u001b[1;36m0.07368106\u001b[0m,\n",
       "                \u001b[1;36m0.0025017066\u001b[0m,\n",
       "                \u001b[1;36m0.038181227\u001b[0m,\n",
       "                \u001b[1;36m-0.07607061\u001b[0m,\n",
       "                \u001b[1;36m-0.03182009\u001b[0m,\n",
       "                \u001b[1;36m0.04869202\u001b[0m,\n",
       "                \u001b[1;36m-0.028599966\u001b[0m,\n",
       "                \u001b[1;36m0.034752943\u001b[0m,\n",
       "                \u001b[1;36m0.027080975\u001b[0m,\n",
       "                \u001b[1;36m0.032496396\u001b[0m,\n",
       "                \u001b[1;36m-0.0036284167\u001b[0m,\n",
       "                \u001b[1;36m-0.04125114\u001b[0m,\n",
       "                \u001b[1;36m0.042432863\u001b[0m,\n",
       "                \u001b[1;36m0.046958406\u001b[0m,\n",
       "                \u001b[1;36m-0.008400537\u001b[0m,\n",
       "                \u001b[1;36m0.026365466\u001b[0m,\n",
       "                \u001b[1;36m-0.07602573\u001b[0m,\n",
       "                \u001b[1;36m-0.07147453\u001b[0m,\n",
       "                \u001b[1;36m0.03766765\u001b[0m,\n",
       "                \u001b[1;36m-0.08318256\u001b[0m,\n",
       "                \u001b[1;36m0.036861774\u001b[0m,\n",
       "                \u001b[1;36m-0.0061818687\u001b[0m,\n",
       "                \u001b[1;36m-0.021962399\u001b[0m,\n",
       "                \u001b[1;36m0.05041582\u001b[0m,\n",
       "                \u001b[1;36m0.05621483\u001b[0m,\n",
       "                \u001b[1;36m-0.037829414\u001b[0m,\n",
       "                \u001b[1;36m0.09600329\u001b[0m,\n",
       "                \u001b[1;36m-0.017125007\u001b[0m,\n",
       "                \u001b[1;36m0.03444506\u001b[0m,\n",
       "                \u001b[1;36m-0.032913096\u001b[0m,\n",
       "                \u001b[1;36m0.099935316\u001b[0m,\n",
       "                \u001b[1;36m0.017872145\u001b[0m,\n",
       "                \u001b[1;36m-0.096478194\u001b[0m,\n",
       "                \u001b[1;36m-0.0055429\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mscores\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m, \u001b[1;36m0.5663027820679463\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can also query the vector db directly\n",
    "db_response = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=prompt,\n",
    ")\n",
    "rich.print(db_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f154ae2",
   "metadata": {},
   "source": [
    "## Creating context joining all the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f07682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_context = tool_response.content\n",
    "prompt_context = \"\\n\".join([c.content for c in db_response.chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a97855fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'system'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant.'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and (b) the memory constraints of your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[\\'q_proj\\', \\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increase our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">minor.\\n\\nLet\\'s run this experiment. We can also increase alpha (in general it is good practice to scale alpha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rank together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--config llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n,\\n    and (b) the memory constraints of your hardware.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: [\\'q_proj\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'v_proj\\']\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our max memory,\\nbut as long as we keep :code:`rank&lt;&lt;embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">run this experiment. We can also increase alpha (in general it is good practice to scale alpha and rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">together).\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">llama2/7B_lora \\\\\\n    lora_attn_modules=[\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'] \\\\\\n    lora_rank=32 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">lora_alpha=64 output_dir=./lora_experiment_1\\n\\nA comparison of the (smoothed) loss curves between this run and our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline over the first 500 steps can be seen below.\\n\\n.. image:: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&amp;B. You can use</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">will need to install W&amp;B and setup an account separately. For more details on\\n    using W&amp;B in torchtune, see our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">commodity GPUs which support `bfloat16 &lt;https://\\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'system'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'You are a helpful assistant.'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m,\n",
       "        \u001b[32m'content'\u001b[0m: \u001b[32m'\\nPlease answer the given query using the context below.\\n\\nQUERY:\\nWhat are the top 5 topics \u001b[0m\n",
       "\u001b[32mthat were explained? Only list succinct bullet points.\\n\\nCONTEXT:\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your \u001b[0m\n",
       "\u001b[32mhardware.\\n\\nThe preceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to \u001b[0m\n",
       "\u001b[32mexperiment a bit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. \u001b[0m\n",
       "\u001b[32mcode-block:: yaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA \u001b[0m\n",
       "\u001b[32mto Q and V projections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply \u001b[0m\n",
       "\u001b[32mLoRA to all linear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to\u001b[0m\n",
       "\u001b[32mincrease our max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively \u001b[0m\n",
       "\u001b[32mminor.\\n\\nLet\\'s run this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and\u001b[0m\n",
       "\u001b[32mrank together\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed \u001b[0m\n",
       "\u001b[32m--config llama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\n",
       "\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between \u001b[0m\n",
       "\u001b[32mthis run and our baseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n,\\n    and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m the memory constraints of your hardware.\\n\\nThe \u001b[0m\n",
       "\u001b[32mpreceding command will run a LoRA finetune with torchtune\\'s factory settings, but we may want to experiment a \u001b[0m\n",
       "\u001b[32mbit.\\nLet\\'s take a closer look at some of the :code:`lora_finetune_distributed` config.\\n\\n.. code-block:: \u001b[0m\n",
       "\u001b[32myaml\\n\\n  # Model Arguments\\n  model:\\n    _component_: lora_llama2_7b\\n    lora_attn_modules: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\', \u001b[0m\n",
       "\u001b[32m\\'v_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    lora_rank: 8\\n    lora_alpha: 16\\n  ...\\n\\nWe see that the default is to apply LoRA to Q and V \u001b[0m\n",
       "\u001b[32mprojections with a rank of 8.\\nSome experiments with LoRA have found that it can be beneficial to apply LoRA to all\u001b[0m\n",
       "\u001b[32mlinear layers in\\nthe self-attention, and to increase the rank to 16 or 32. Note that this is likely to increase \u001b[0m\n",
       "\u001b[32mour max memory,\\nbut as long as we keep :code:`rank<<embed_dim`, the impact should be relatively minor.\\n\\nLet\\'s \u001b[0m\n",
       "\u001b[32mrun this experiment. We can also increase alpha \u001b[0m\u001b[32m(\u001b[0m\u001b[32min general it is good practice to scale alpha and rank \u001b[0m\n",
       "\u001b[32mtogether\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n.. code-block:: bash\\n\\n    tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config\u001b[0m\n",
       "\u001b[32mllama2/7B_lora \\\\\\n    \u001b[0m\u001b[32mlora_attn_modules\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'output_proj\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \\\\\\n    \u001b[0m\u001b[32mlora_rank\u001b[0m\u001b[32m=\u001b[0m\u001b[32m32\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mlora_alpha\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m \u001b[0m\u001b[32moutput_dir\u001b[0m\u001b[32m=./lora_experiment_1\\n\\nA comparison of the \u001b[0m\u001b[32m(\u001b[0m\u001b[32msmoothed\u001b[0m\u001b[32m)\u001b[0m\u001b[32m loss curves between this run and our\u001b[0m\n",
       "\u001b[32mbaseline over the first 500 steps can be seen below.\\n\\n.. image:: \u001b[0m\n",
       "\u001b[32m/_static/img/lora_experiment_loss_curves.png\\n\\n.. note::\\n    The above figure was generated with W&B. You can use\u001b[0m\n",
       "\u001b[32mtorchtune\\'s :class:`~torchtune.training.metric_logging.WandBLogger`\\n    to generate similar loss curves, but you \u001b[0m\n",
       "\u001b[32mwill need to install W&B and setup an account separately. For more details on\\n    using W&B in torchtune, see our \u001b[0m\n",
       "\u001b[32m\":ref:`wandb_logging`\" recipe.\\n\\n.. _lora_tutorial_memory_tradeoff_label:\\n\\nTrading off memory and model \u001b[0m\n",
       "\u001b[32mperformance with LoRA\\n--------------------------------------------------\\n\\nIn the preceding example, we ran LoRA \u001b[0m\n",
       "\u001b[32mon two devices. But given LoRA\\'s low memory footprint, we can run fine-tuning\\non a single device using most \u001b[0m\n",
       "\u001b[32mcommodity GPUs which support `bfloat16 <https://\\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "extended_prompt = f\"\"\"\n",
    "Please answer the given query using the context below.\n",
    "\n",
    "QUERY:\n",
    "{prompt}\n",
    "\n",
    "CONTEXT:\n",
    "{prompt_context}\n",
    "\"\"\"\n",
    "messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "rich.print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "526a6848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">completion_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here are the top 5 topics that were explained:\\n\\n* LoRA (Low-Rank Adaptation)\\n* Memory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configurations\\n* Trading off memory and model performance with LoRA'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1592.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Metric</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1660.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">unit</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletionResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcompletion_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Here are the top 5 topics that were explained:\\n\\n* LoRA \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLow-Rank Adaptation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Memory \u001b[0m\n",
       "\u001b[32mconstraints of hardware\\n* Fine-tuning with LoRA on a single device\\n* Experimenting with different LoRA \u001b[0m\n",
       "\u001b[32mconfigurations\\n* Trading off memory and model performance with LoRA'\u001b[0m,\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'prompt_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1592\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'completion_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m68\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mMetric\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmetric\u001b[0m=\u001b[32m'total_tokens'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1;36m1660\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33munit\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=messages,\n",
    "    model_id=model_name,\n",
    "    timeout=600\n",
    ")\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ace08",
   "metadata": {},
   "source": [
    "### Using Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "437e8ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Left as an extra exercise for the reader\n",
    "from llama_stack_client import Agent\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client, \n",
    "    model=model_name,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools = [\n",
    "        {\n",
    "          \"name\": \"builtin::rag/knowledge_search\",\n",
    "          \"args\" : {\n",
    "            \"vector_db_ids\": [vector_db_id],\n",
    "          }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a80342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/0e7bbf74-c3bc-4f7a-9b35-789eb7d4095c/session \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">User&gt; What is Lora?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "User> What is Lora?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/0e7bbf74-c3bc-4f7a-9b35-789eb7d4095c/session/606fcb14-b6b8-4c0c-b4cf-ace08c607135/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mL\u001b[0m\u001b[33mora\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Lora'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: .\\n\\n*Sounds great! How do I use it?*\\n\\nYou can finetune using any of our recipes with the ``lora_`` prefix, e.g. :ref:`lora_finetune_single_device<lora_finetune_recipe_label>`. These recipes utilize\\nLoRA-enabled model builders, which we support for all our models, and also use the ``lora_`` prefix, e.g.\\nthe :func:`torchtune.models.llama3.llama3` model has a corresponding :func:`torchtune.models.llama3.lora_llama3`.\\nWe aim to provide a comprehensive set of configurations to allow you to get started with training with LoRA quickly,\\njust specify any config with ``_lora`` in its name, e.g:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device --config llama3/8B_lora_single_device\\n\\n\\nThere are two sets of parameters to customize LoRA to suit your needs. Firstly, the parameters which control\\nwhich linear layers LoRA should be applied to in the model:\\n\\n* ``lora_attn_modules: list[str]`` accepts a list of strings specifying which layers of the model to apply\\n  LoRA to:\\n\\n  * ``q_proj`` applies LoRA to the query projection layer.\\n  * ``k_proj`` applies LoRA to the key projection layer.\\n  * ``v_proj`` applies LoRA to the value projection layer.\\n  * ``output_proj`` applies LoRA to the attention output projection layer.\\n\\n  Whilst adding more layers to be fine-tuned may improve model accuracy,\\n  this will come at the cost of increased memory usage and reduced training speed.\\n\\n* ``apply_lora_to_mlp: Bool`` applies LoRA to the MLP in each transformer layer.\\n* ``apply_lora_to_output: Bool`` applies LoRA to the model's final output projection.\\n  This is usually a projection to vocabulary space (e.g. in language models), but\\n  other modelling tasks may have different projections - classifier models will project\\n  to the number of classes, for example\\n\\n.. note::\\n\\n  Models which use tied embeddings (such as Gemma and Qwen2 1.5B and 0.5B) for the\\n  final output projection do not support ``apply_lora_to_output``.\\n\\nThese are all specified under the ``model`` flag or config entry, i.e:\\n\\n.. code-block:: bash\\n\\n  tune run lora_finetune_single_device\\nMetadata: {'document_id': 'num-2'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\nMetadata: {'document_id': 'num-3'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: .. _lora_finetune_label:\\n\\n============================\\nFine-Tuning Llama2 with LoRA\\n============================\\n\\nThis guide will teach you about `LoRA <https://arxiv.org/abs/2106.09685>`_, a parameter-efficient finetuning technique,\\nand show you how you can use torchtune to finetune a Llama2 model with LoRA.\\nIf you already know what LoRA is and want to get straight to running\\nyour own LoRA finetune in torchtune, you can jump to :ref:`LoRA finetuning recipe in torchtune<lora_recipe_label>`.\\n\\n.. grid:: 2\\n\\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\\n\\n      * What LoRA is and how it saves memory during finetuning\\n      * An overview of LoRA components in torchtune\\n      * How to run a LoRA finetune using torchtune\\n      * How to experiment with different LoRA configurations\\n\\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\\n\\n      * Be familiar with :ref:`torchtune<overview_label>`\\n      * Make sure to :ref:`install torchtune<install_label>`\\n      * Make sure you have downloaded the :ref:`Llama2-7B model weights<download_llama_label>`\\n\\nWhat is LoRA?\\n-------------\\n\\n`LoRA <https://arxiv.org/abs/2106.09685>`_ is an adapter-based method for\\nparameter-efficient finetuning that adds trainable low-rank decomposition matrices to different layers of a neural network,\\nthen freezes the network's remaining parameters. LoRA is most commonly applied to\\ntransformer models, in which case it is common to add the low-rank matrices\\nto some of the linear projections in each transformer layer's self-attention.\\n\\n.. note::\\n\\n    If you're unfamiliar, check out these references for the `definition of rank <https://en.wikipedia.org/wiki/Rank_(linear_algebra)>`_\\n    and discussion of `low-rank approximations <https://en.wikipedia.org/wiki/Low-rank_approximation>`_.\\n\\nBy finetuning with LoRA (as opposed to finetuning all model parameters),\\nyou can expect to see memory savings due to a substantial reduction in the\\nnumber of parameters with gradients. When using an optimizer with momentum,\\nlike `AdamW <https://py\\nMetadata: {'document_id': 'num-3'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Lora\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m rewritten\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m text\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m readable\u001b[0m\u001b[33m format\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mWhat\u001b[0m\u001b[33m is\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m?\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLow\u001b[0m\u001b[33m-R\u001b[0m\u001b[33mank\u001b[0m\u001b[33m Adapt\u001b[0m\u001b[33mation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m is\u001b[0m\u001b[33m an\u001b[0m\u001b[33m adapter\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m method\u001b[0m\u001b[33m for\u001b[0m\u001b[33m parameter\u001b[0m\u001b[33m-efficient\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m that\u001b[0m\u001b[33m adds\u001b[0m\u001b[33m trainable\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-r\u001b[0m\u001b[33mank\u001b[0m\u001b[33m decomposition\u001b[0m\u001b[33m matrices\u001b[0m\u001b[33m to\u001b[0m\u001b[33m different\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m neural\u001b[0m\u001b[33m network\u001b[0m\u001b[33m,\u001b[0m\u001b[33m then\u001b[0m\u001b[33m freezes\u001b[0m\u001b[33m the\u001b[0m\u001b[33m network\u001b[0m\u001b[33m's\u001b[0m\u001b[33m remaining\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m technique\u001b[0m\u001b[33m is\u001b[0m\u001b[33m commonly\u001b[0m\u001b[33m applied\u001b[0m\u001b[33m to\u001b[0m\u001b[33m transformer\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m where\u001b[0m\u001b[33m it\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m adding\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-r\u001b[0m\u001b[33mank\u001b[0m\u001b[33m matrices\u001b[0m\u001b[33m to\u001b[0m\u001b[33m some\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m linear\u001b[0m\u001b[33m projections\u001b[0m\u001b[33m in\u001b[0m\u001b[33m each\u001b[0m\u001b[33m transformer\u001b[0m\u001b[33m layer\u001b[0m\u001b[33m's\u001b[0m\u001b[33m self\u001b[0m\u001b[33m-\u001b[0m\u001b[33mattention\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mHow\u001b[0m\u001b[33m does\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m work\u001b[0m\u001b[33m?\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mLo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m works\u001b[0m\u001b[33m by\u001b[0m\u001b[33m adding\u001b[0m\u001b[33m trainable\u001b[0m\u001b[33m low\u001b[0m\u001b[33m-r\u001b[0m\u001b[33mank\u001b[0m\u001b[33m decomposition\u001b[0m\u001b[33m matrices\u001b[0m\u001b[33m to\u001b[0m\u001b[33m different\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m neural\u001b[0m\u001b[33m network\u001b[0m\u001b[33m.\u001b[0m\u001b[33m These\u001b[0m\u001b[33m matrices\u001b[0m\u001b[33m are\u001b[0m\u001b[33m learned\u001b[0m\u001b[33m during\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m and\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m approximate\u001b[0m\u001b[33m the\u001b[0m\u001b[33m original\u001b[0m\u001b[33m weights\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m network\u001b[0m\u001b[33m.\u001b[0m\u001b[33m By\u001b[0m\u001b[33m freezing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m remaining\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m reduces\u001b[0m\u001b[33m the\u001b[0m\u001b[33m number\u001b[0m\u001b[33m of\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m with\u001b[0m\u001b[33m gradients\u001b[0m\u001b[33m,\u001b[0m\u001b[33m resulting\u001b[0m\u001b[33m in\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m savings\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mBenefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mBy\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m instead\u001b[0m\u001b[33m of\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m all\u001b[0m\u001b[33m model\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m expect\u001b[0m\u001b[33m to\u001b[0m\u001b[33m see\u001b[0m\u001b[33m significant\u001b[0m\u001b[33m memory\u001b[0m\u001b[33m savings\u001b[0m\u001b[33m due\u001b[0m\u001b[33m to\u001b[0m\u001b[33m a\u001b[0m\u001b[33m substantial\u001b[0m\u001b[33m reduction\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m number\u001b[0m\u001b[33m of\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m with\u001b[0m\u001b[33m gradients\u001b[0m\u001b[33m.\u001b[0m\u001b[33m When\u001b[0m\u001b[33m using\u001b[0m\u001b[33m an\u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m with\u001b[0m\u001b[33m momentum\u001b[0m\u001b[33m,\u001b[0m\u001b[33m like\u001b[0m\u001b[33m Adam\u001b[0m\u001b[33mW\u001b[0m\u001b[33m,\u001b[0m\u001b[33m this\u001b[0m\u001b[33m can\u001b[0m\u001b[33m lead\u001b[0m\u001b[33m to\u001b[0m\u001b[33m faster\u001b[0m\u001b[33m training\u001b[0m\u001b[33m times\u001b[0m\u001b[33m and\u001b[0m\u001b[33m improved\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mFine\u001b[0m\u001b[33m-T\u001b[0m\u001b[33muning\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m2\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m a\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m2\u001b[0m\u001b[33m model\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m using\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m,\u001b[0m\u001b[33m follow\u001b[0m\u001b[33m these\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Install\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m and\u001b[0m\u001b[33m download\u001b[0m\u001b[33m the\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m2\u001b[0m\u001b[33m-\u001b[0m\u001b[33m7\u001b[0m\u001b[33mB\u001b[0m\u001b[33m model\u001b[0m\u001b[33m weights\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Load\u001b[0m\u001b[33m the\u001b[0m\u001b[33m pre\u001b[0m\u001b[33m-trained\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m2\u001b[0m\u001b[33m model\u001b[0m\u001b[33m and\u001b[0m\u001b[33m add\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m adapter\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m desired\u001b[0m\u001b[33m layers\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Define\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m configuration\u001b[0m\u001b[33m in\u001b[0m\u001b[33m your\u001b[0m\u001b[33m experiment\u001b[0m\u001b[33m,\u001b[0m\u001b[33m including\u001b[0m\u001b[33m the\u001b[0m\u001b[33m number\u001b[0m\u001b[33m of\u001b[0m\u001b[33m ranks\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m optimizer\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Run\u001b[0m\u001b[33m the\u001b[0m\u001b[33m experiment\u001b[0m\u001b[33m using\u001b[0m\u001b[33m torch\u001b[0m\u001b[33mt\u001b[0m\u001b[33mune\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mExperiment\u001b[0m\u001b[33ming\u001b[0m\u001b[33m with\u001b[0m\u001b[33m Different\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m Config\u001b[0m\u001b[33murations\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m get\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m out\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Lo\u001b[0m\u001b[33mRA\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m experiment\u001b[0m\u001b[33m with\u001b[0m\u001b[33m different\u001b[0m\u001b[33m configurations\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Changing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m number\u001b[0m\u001b[33m of\u001b[0m\u001b[33m ranks\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m different\u001b[0m\u001b[33m optim\u001b[0m\u001b[33mizers\u001b[0m\u001b[33m (\u001b[0m\u001b[33me\u001b[0m\u001b[33m.g\u001b[0m\u001b[33m.,\u001b[0m\u001b[33m Adam\u001b[0m\u001b[33mW\u001b[0m\u001b[33m)\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Adjust\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m rate\u001b[0m\u001b[33m and\u001b[0m\u001b[33m batch\u001b[0m\u001b[33m size\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33mBy\u001b[0m\u001b[33m trying\u001b[0m\u001b[33m out\u001b[0m\u001b[33m these\u001b[0m\u001b[33m different\u001b[0m\u001b[33m configurations\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m find\u001b[0m\u001b[33m the\u001b[0m\u001b[33m optimal\u001b[0m\u001b[33m settings\u001b[0m\u001b[33m for\u001b[0m\u001b[33m your\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m use\u001b[0m\u001b[33m case\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "user_prompts = [\n",
    "    \"What is Lora?\"\n",
    "]\n",
    "session_id = rag_agent.create_session(f\"rag session-{uuid.uuid4()}\")\n",
    "for prompt in user_prompts:\n",
    "    rich.print(f\"User> {prompt}\")\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
